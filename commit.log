commit fcf999f068ee50bd093a2bf23aaee9996b320f6f
Merge: 39c4d72 fc3afc7
Author: Wang Qi <wangqi@users.noreply.github.com>
Date:   Fri Dec 26 20:08:48 2025 -0500

    Merge branch 'ml-explore:main' into main

commit fc3afc7cdbc4b6120d210c4c58c6b132ce346775
Author: Tim Sneath <tim@sneath.org>
Date:   Thu Dec 11 10:30:07 2025 -0800

    Update LLMEval example (#452)
    
    * Update LLMEval example

diff --git a/Applications/LLMEval/AppIcon.icon/Assets/bubbles3.png b/Applications/LLMEval/AppIcon.icon/Assets/bubbles3.png
new file mode 100644
index 0000000..ea1d270
Binary files /dev/null and b/Applications/LLMEval/AppIcon.icon/Assets/bubbles3.png differ
diff --git a/Applications/LLMEval/AppIcon.icon/icon.json b/Applications/LLMEval/AppIcon.icon/icon.json
new file mode 100644
index 0000000..01bb05b
--- /dev/null
+++ b/Applications/LLMEval/AppIcon.icon/icon.json
@@ -0,0 +1,46 @@
+{
+  "fill" : {
+    "automatic-gradient" : "srgb:0.66275,0.79216,0.87451,1.00000",
+    "orientation" : {
+      "start" : {
+        "x" : 0.5,
+        "y" : 0
+      },
+      "stop" : {
+        "x" : 0.5,
+        "y" : 0.7
+      }
+    }
+  },
+  "groups" : [
+    {
+      "layers" : [
+        {
+          "image-name" : "bubbles3.png",
+          "name" : "bubbles3",
+          "position" : {
+            "scale" : 1.35,
+            "translation-in-points" : [
+              0,
+              0
+            ]
+          }
+        }
+      ],
+      "shadow" : {
+        "kind" : "neutral",
+        "opacity" : 0.5
+      },
+      "translucency" : {
+        "enabled" : true,
+        "value" : 0.5
+      }
+    }
+  ],
+  "supported-platforms" : {
+    "circles" : [
+      "watchOS"
+    ],
+    "squares" : "shared"
+  }
+}
\ No newline at end of file
diff --git a/Applications/LLMEval/AssetCatalog.xcassets/AccentColor.colorset/Contents.json b/Applications/LLMEval/AssetCatalog.xcassets/AccentColor.colorset/Contents.json
new file mode 100644
index 0000000..dbb0321
--- /dev/null
+++ b/Applications/LLMEval/AssetCatalog.xcassets/AccentColor.colorset/Contents.json
@@ -0,0 +1,20 @@
+{
+  "colors" : [
+    {
+      "color" : {
+        "color-space" : "srgb",
+        "components" : {
+          "alpha" : "1.000",
+          "blue" : "1.000",
+          "green" : "0.689",
+          "red" : "0.307"
+        }
+      },
+      "idiom" : "universal"
+    }
+  ],
+  "info" : {
+    "author" : "xcode",
+    "version" : 1
+  }
+}
diff --git a/Applications/LLMEval/Assets.xcassets/Contents.json b/Applications/LLMEval/AssetCatalog.xcassets/Contents.json
similarity index 100%
rename from Applications/LLMEval/Assets.xcassets/Contents.json
rename to Applications/LLMEval/AssetCatalog.xcassets/Contents.json
diff --git a/Applications/LLMEval/Assets.xcassets/AccentColor.colorset/Contents.json b/Applications/LLMEval/Assets.xcassets/AccentColor.colorset/Contents.json
deleted file mode 100644
index eb87897..0000000
--- a/Applications/LLMEval/Assets.xcassets/AccentColor.colorset/Contents.json
+++ /dev/null
@@ -1,11 +0,0 @@
-{
-  "colors" : [
-    {
-      "idiom" : "universal"
-    }
-  ],
-  "info" : {
-    "author" : "xcode",
-    "version" : 1
-  }
-}
diff --git a/Applications/LLMEval/Assets.xcassets/AppIcon.appiconset/Contents.json b/Applications/LLMEval/Assets.xcassets/AppIcon.appiconset/Contents.json
deleted file mode 100644
index 532cd72..0000000
--- a/Applications/LLMEval/Assets.xcassets/AppIcon.appiconset/Contents.json
+++ /dev/null
@@ -1,63 +0,0 @@
-{
-  "images" : [
-    {
-      "idiom" : "universal",
-      "platform" : "ios",
-      "size" : "1024x1024"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "1x",
-      "size" : "16x16"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "2x",
-      "size" : "16x16"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "1x",
-      "size" : "32x32"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "2x",
-      "size" : "32x32"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "1x",
-      "size" : "128x128"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "2x",
-      "size" : "128x128"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "1x",
-      "size" : "256x256"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "2x",
-      "size" : "256x256"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "1x",
-      "size" : "512x512"
-    },
-    {
-      "idiom" : "mac",
-      "scale" : "2x",
-      "size" : "512x512"
-    }
-  ],
-  "info" : {
-    "author" : "xcode",
-    "version" : 1
-  }
-}
diff --git a/Applications/LLMEval/ContentView.swift b/Applications/LLMEval/ContentView.swift
deleted file mode 100644
index fbbb472..0000000
--- a/Applications/LLMEval/ContentView.swift
+++ /dev/null
@@ -1,386 +0,0 @@
-// Copyright © 2024 Apple Inc.
-
-import AsyncAlgorithms
-import MLX
-import MLXLLM
-import MLXLMCommon
-import MarkdownUI
-import Metal
-import SwiftUI
-import Tokenizers
-
-struct ContentView: View {
-    @Environment(DeviceStat.self) private var deviceStat
-
-    @State var llm = LLMEvaluator()
-
-    enum displayStyle: String, CaseIterable, Identifiable {
-        case plain, markdown
-        var id: Self { self }
-    }
-
-    @State private var selectedDisplayStyle = displayStyle.markdown
-
-    var body: some View {
-        VStack(alignment: .leading) {
-            VStack {
-                HStack {
-                    Text(llm.modelInfo)
-                        .textFieldStyle(.roundedBorder)
-
-                    Spacer()
-
-                    Text(llm.stat)
-                }
-                HStack {
-                    Toggle(isOn: $llm.includeWeatherTool) {
-                        Text("Include tools")
-                    }
-                    .frame(maxWidth: 350, alignment: .leading)
-                    Toggle(isOn: $llm.enableThinking) {
-                        Text("Thinking")
-                            .help(
-                                "Switches between thinking and non-thinking modes. Support: Qwen3")
-                    }
-                    Spacer()
-                    if llm.running {
-                        ProgressView()
-                            .frame(maxHeight: 20)
-                        Spacer()
-                    }
-                    Picker("", selection: $selectedDisplayStyle) {
-                        ForEach(displayStyle.allCases, id: \.self) { option in
-                            Text(option.rawValue.capitalized)
-                                .tag(option)
-                        }
-
-                    }
-                    .pickerStyle(.segmented)
-                    #if os(visionOS)
-                        .frame(maxWidth: 250)
-                    #else
-                        .frame(maxWidth: 150)
-                    #endif
-                }
-            }
-
-            // show the model output
-            ScrollView(.vertical) {
-                ScrollViewReader { sp in
-                    Group {
-                        if selectedDisplayStyle == .plain {
-                            Text(llm.output)
-                                .textSelection(.enabled)
-                        } else {
-                            Markdown(llm.output)
-                                .textSelection(.enabled)
-                        }
-                    }
-                    .onChange(of: llm.output) { _, _ in
-                        sp.scrollTo("bottom")
-                    }
-
-                    Spacer()
-                        .frame(width: 1, height: 1)
-                        .id("bottom")
-                }
-            }
-
-            HStack {
-                TextField("prompt", text: Bindable(llm).prompt)
-                    .onSubmit(generate)
-                    .disabled(llm.running)
-                    #if os(visionOS)
-                        .textFieldStyle(.roundedBorder)
-                    #endif
-                Button(llm.running ? "stop" : "generate", action: llm.running ? cancel : generate)
-            }
-        }
-        #if os(visionOS)
-            .padding(40)
-        #else
-            .padding()
-        #endif
-        .toolbar {
-            ToolbarItem {
-                Label(
-                    "Memory Usage: \(deviceStat.gpuUsage.activeMemory.formatted(.byteCount(style: .memory)))",
-                    systemImage: "info.circle.fill"
-                )
-                .labelStyle(.titleAndIcon)
-                .padding(.horizontal)
-                .help(
-                    Text(
-                        """
-                        Active Memory: \(deviceStat.gpuUsage.activeMemory.formatted(.byteCount(style: .memory)))/\(GPU.memoryLimit.formatted(.byteCount(style: .memory)))
-                        Cache Memory: \(deviceStat.gpuUsage.cacheMemory.formatted(.byteCount(style: .memory)))/\(GPU.cacheLimit.formatted(.byteCount(style: .memory)))
-                        Peak Memory: \(deviceStat.gpuUsage.peakMemory.formatted(.byteCount(style: .memory)))
-                        """
-                    )
-                )
-            }
-            ToolbarItem(placement: .primaryAction) {
-                Button {
-                    Task {
-                        copyToClipboard(llm.output)
-                    }
-                } label: {
-                    Label("Copy Output", systemImage: "doc.on.doc.fill")
-                }
-                .disabled(llm.output == "")
-                .labelStyle(.titleAndIcon)
-            }
-
-        }
-        .task {
-            do {
-                // pre-load the weights on launch to speed up the first generation
-                _ = try await llm.load()
-            } catch {
-                llm.output = "Failed: \(error)"
-            }
-        }
-    }
-
-    private func generate() {
-        llm.generate()
-    }
-
-    private func cancel() {
-        llm.cancelGeneration()
-    }
-
-    private func copyToClipboard(_ string: String) {
-        #if os(macOS)
-            NSPasteboard.general.clearContents()
-            NSPasteboard.general.setString(string, forType: .string)
-        #else
-            UIPasteboard.general.string = string
-        #endif
-    }
-}
-
-@Observable
-@MainActor
-class LLMEvaluator {
-
-    var running = false
-
-    var includeWeatherTool = false
-    var enableThinking = false
-
-    var prompt = ""
-    var output = ""
-    var modelInfo = ""
-    var stat = ""
-
-    /// This controls which model loads. `qwen2_5_1_5b` is one of the smaller ones, so this will fit on
-    /// more devices.
-    let modelConfiguration = LLMRegistry.qwen3_1_7b_4bit
-
-    /// parameters controlling the output
-    let generateParameters = GenerateParameters(maxTokens: 240, temperature: 0.6)
-    let updateInterval = Duration.seconds(0.25)
-
-    /// A task responsible for handling the generation process.
-    var generationTask: Task<Void, Error>?
-
-    enum LoadState {
-        case idle
-        case loaded(ModelContainer)
-    }
-
-    var loadState = LoadState.idle
-
-    let currentWeatherTool = Tool<WeatherInput, WeatherOutput>(
-        name: "get_current_weather",
-        description: "Get the current weather in a given location",
-        parameters: [
-            .required(
-                "location", type: .string, description: "The city and state, e.g. San Francisco, CA"
-            ),
-            .optional(
-                "unit",
-                type: .string,
-                description: "The unit of temperature",
-                extraProperties: [
-                    "enum": ["celsius", "fahrenheit"],
-                    "default": "celsius",
-                ]
-            ),
-        ]
-    ) { input in
-        let range = input.unit == "celsius" ? (min: -20.0, max: 40.0) : (min: 0, max: 100)
-        let temperature = Double.random(in: range.min ... range.max)
-
-        let conditions = ["Sunny", "Cloudy", "Rainy", "Snowy", "Windy", "Stormy"].randomElement()!
-
-        return WeatherOutput(temperature: temperature, conditions: conditions)
-    }
-
-    let addTool = Tool<AddInput, AddOutput>(
-        name: "add_two_numbers",
-        description: "Add two numbers together",
-        parameters: [
-            .required("first", type: .int, description: "The first number to add"),
-            .required("second", type: .int, description: "The second number to add"),
-        ]
-    ) { input in
-        AddOutput(result: input.first + input.second)
-    }
-
-    let timeTool = Tool<EmptyInput, TimeOutput>(
-        name: "get_time",
-        description: "Get the current time",
-        parameters: []
-    ) { _ in
-        TimeOutput(time: Date.now.formatted())
-    }
-
-    /// load and return the model -- can be called multiple times, subsequent calls will
-    /// just return the loaded model
-    func load() async throws -> ModelContainer {
-        switch loadState {
-        case .idle:
-            // limit the buffer cache
-            MLX.GPU.set(cacheLimit: 20 * 1024 * 1024)
-
-            let modelContainer = try await LLMModelFactory.shared.loadContainer(
-                configuration: modelConfiguration
-            ) {
-                [modelConfiguration] progress in
-                Task { @MainActor in
-                    self.modelInfo =
-                        "Downloading \(modelConfiguration.name): \(Int(progress.fractionCompleted * 100))%"
-                }
-            }
-            let numParams = await modelContainer.perform { context in
-                context.model.numParameters()
-            }
-
-            self.prompt = modelConfiguration.defaultPrompt
-            self.modelInfo =
-                "Loaded \(modelConfiguration.id). Weights: \(numParams / (1024*1024))M"
-            loadState = .loaded(modelContainer)
-            return modelContainer
-
-        case .loaded(let modelContainer):
-            return modelContainer
-        }
-    }
-
-    private func generate(prompt: String, toolResult: String? = nil) async {
-
-        self.output = ""
-        var chat: [Chat.Message] = [
-            .system("You are a helpful assistant"),
-            .user(prompt),
-        ]
-
-        if let toolResult {
-            chat.append(.tool(toolResult))
-        }
-
-        let userInput = UserInput(
-            chat: chat,
-            tools: includeWeatherTool
-                ? [currentWeatherTool.schema, addTool.schema, timeTool.schema] : nil,
-            additionalContext: ["enable_thinking": enableThinking]
-        )
-
-        do {
-            let modelContainer = try await load()
-
-            // each time you generate you will get something new
-            MLXRandom.seed(UInt64(Date.timeIntervalSinceReferenceDate * 1000))
-
-            try await modelContainer.perform { (context: ModelContext) -> Void in
-                let lmInput = try await context.processor.prepare(input: userInput)
-                let stream = try MLXLMCommon.generate(
-                    input: lmInput, parameters: generateParameters, context: context)
-
-                // generate and output in batches
-                for await batch in stream._throttle(
-                    for: updateInterval, reducing: Generation.collect)
-                {
-                    let output = batch.compactMap { $0.chunk }.joined(separator: "")
-                    if !output.isEmpty {
-                        Task { @MainActor [output] in
-                            self.output += output
-                        }
-                    }
-
-                    if let completion = batch.compactMap({ $0.info }).first {
-                        Task { @MainActor in
-                            self.stat = "\(completion.tokensPerSecond) tokens/s"
-                        }
-                    }
-
-                    if let toolCall = batch.compactMap({ $0.toolCall }).first {
-                        try await handleToolCall(toolCall, prompt: prompt)
-                    }
-                }
-            }
-
-        } catch {
-            output = "Failed: \(error)"
-        }
-    }
-
-    func generate() {
-        guard !running else { return }
-        let currentPrompt = prompt
-        prompt = ""
-        generationTask = Task {
-            running = true
-            await generate(prompt: currentPrompt)
-            running = false
-        }
-    }
-
-    func cancelGeneration() {
-        generationTask?.cancel()
-        running = false
-    }
-
-    private func handleToolCall(_ toolCall: ToolCall, prompt: String) async throws {
-        let result =
-            switch toolCall.function.name {
-            case currentWeatherTool.name:
-                try await toolCall.execute(with: currentWeatherTool).toolResult
-            case addTool.name:
-                try await toolCall.execute(with: addTool).toolResult
-            case timeTool.name:
-                try await toolCall.execute(with: timeTool).toolResult
-            default:
-                "No tool match"
-            }
-
-        await generate(prompt: prompt, toolResult: result)
-    }
-}
-
-struct WeatherInput: Codable {
-    let location: String
-    let unit: String?
-}
-
-struct WeatherOutput: Codable {
-    let temperature: Double
-    let conditions: String
-}
-
-struct AddInput: Codable {
-    let first: Int
-    let second: Int
-}
-
-struct AddOutput: Codable {
-    let result: Int
-}
-
-struct EmptyInput: Codable {}
-
-struct TimeOutput: Codable {
-    let time: String
-}
diff --git a/Applications/LLMEval/LLMEval.entitlements b/Applications/LLMEval/LLMEval.entitlements
index 0ec64e4..5f13871 100644
--- a/Applications/LLMEval/LLMEval.entitlements
+++ b/Applications/LLMEval/LLMEval.entitlements
@@ -6,8 +6,6 @@
 	<true/>
 	<key>com.apple.security.app-sandbox</key>
 	<true/>
-	<key>com.apple.security.device.usb</key>
-	<true/>
 	<key>com.apple.security.files.user-selected.read-only</key>
 	<true/>
 	<key>com.apple.security.network.client</key>
diff --git a/Applications/LLMEval/Models/CarKeysStory.md b/Applications/LLMEval/Models/CarKeysStory.md
new file mode 100644
index 0000000..eacbffc
--- /dev/null
+++ b/Applications/LLMEval/Models/CarKeysStory.md
@@ -0,0 +1,237 @@
+Can you summarize this story for me in two paragraphs?
+
+```
+# The Department of Misplaced Car Keys and the International Incident
+
+Arthur Penhaligon, Head of the Department of Misplaced Car Keys (DMCK), believed that a tidy desk was the sign of a tidy mind. His own desk, an immense slab of mahogany that had once belonged to a Victorian-era postmaster, was a testament to this philosophy. Every paper was filed, every pen was in its pot, and the official DMCK rubber stamp was perfectly aligned with the edge of his blotter. It was a small island of order in a sea of bureaucratic chaos.
+
+The DMCK, a largely forgotten sub-department of a ministry so obscure its own employees often forgot its name, had a single, solemn duty: to account for and, if possible, repatriate the millions of car keys that vanished from pockets, handbags, and kitchen counters every year. It was a task of Sisyphean proportions, and one that the rest of Whitehall regarded with a mixture of pity and quiet amusement.
+
+"Anything to report, Agnes?" Arthur asked his secretary, a woman whose formidable demeanor was matched only by her loyalty.
+
+Agnes peered over her spectacles. "A set of Ford Fiesta keys materialized in a bird's nest in Regent's Park, sir. And a lone BMW fob was found in the belly of a cod sold at Billingsgate Market."
+
+"Standard stuff," Arthur murmured, making a note. "Anything... unusual?"
+
+"Just the package from Berlin, sir. It arrived this morning by special courier." She gestured to a small, lead-lined box on the corner of his desk. It was the only thing that marred the perfect symmetry of his workspace.
+
+Arthur's heart gave a little flutter. The Berlin office was a recent, and some said, unnecessary, expansion. He hadn't been consulted. The package was sealed with a wax emblem he didn't recognize, a stern-looking eagle clutching what looked suspiciously like a tiny car key. With a sense of unease, he broke the seal and lifted the lid.
+
+Inside, nestled on a bed of black velvet, was a single, ornate, and impossibly old-fashioned car key. It was silver, with a heavy, intricate head shaped like a double-headed eagle. It looked more like a ceremonial scepter than something you'd use to unlock a hatchback.
+
+Attached was a note, typed on crisp, watermarked paper.
+
+*"Herr Penhaligon,*
+
+*Found this in the back of a taxi. Looks important. Might be yours.*
+
+*Regards,*
+
+*Klaus."*
+
+Arthur stared at the key. It was magnificent. It was also, he was quite certain, the key to the Black Limousine of the German Chancellor. And it was missing.
+
+
+
+
+Panic, a sensation Arthur usually associated with finding a misfiled memo, began to prickle at the back of his neck. The Chancellor of Germany was due in London for sensitive trade negotiations in two days. The image of his motorcade grinding to a halt, the Chancellor unable to get into his own car, flashed through Arthur's mind. It would be a diplomatic incident of catastrophic proportions. The newspapers would have a field day. "KEY-STONE COPS!" the headlines would scream.
+
+"Agnes," he said, his voice a little tight. "Get me the emergency contact for the Foreign Office. The one marked 'Never, Ever, Under Any Circumstances, Use This Number.'"
+
+Agnes, who had seen Arthur handle the Great Key Fob Shortage of '98 and the Phantom Key Jangler of Fleet Street without so much as a ruffled feather, raised an eyebrow. "Is everything alright, sir?"
+
+"No, Agnes. Everything is very much not alright."
+
+Within the hour, a man from the Foreign Office arrived. He didn't so much enter the room as materialize in it, a tall, impossibly thin man in a suit so sharp it could have cut glass. He introduced himself as "Montague," though Arthur had the distinct impression that wasn't his real name.
+
+"Penhaligon," Montague said, his voice a low, confidential murmur. He didn't offer to shake hands. "I was told there was a… situation."
+
+Arthur gestured to the key. Montague picked it up, his fingers clad in thin leather gloves. He examined it for a long, silent moment. "Good Lord," he breathed. "The Chancellor's Key. We had a whisper on the wire that it had gone missing. We thought it was just idle chatter." He looked at Arthur, a flicker of something that might have been respect in his cold eyes. "And it turns up here. In the Department of Misplaced Car Keys. The irony is almost too much to bear."
+
+
+
+
+"Too much to bear or not," Arthur said, his voice regaining some of its usual firmness. "The fact remains, we have it, and they, presumably, do not. The Chancellor arrives on Friday. This needs to be resolved before then, and without a single word of it leaking to the press."
+
+Montague nodded slowly. "The press are the least of our worries. If certain other agencies were to learn of this... let's just say it would be disadvantageous." He began to pace the small office, his long legs covering the distance in three short strides. "We can't simply hand it back. That would be an admission of a security breach. We need a way to return it covertly."
+
+He stopped and looked at Arthur. "You're the expert on misplaced items, Penhaligon. How does a thing like this... get un-misplaced?"
+
+Arthur, who was used to dealing with keys found in gutters and postboxes, felt a surge of professional pride. This was, without a doubt, the most significant misplacement of his career. "We'll need a plan," he said. "Something subtle, something discreet. Something that involves… misdirection."
+
+He steepled his fingers, his mind, usually occupied with the migratory patterns of lost Vauxhall keys, now whirring with the kind of intricate plotting usually reserved for spy novels. "Agnes," he called out. "Bring me the file on the 'Phantom Thread' technique. And get me Henderson from Applied Magnetism."
+
+Henderson was the DMCK's resident Q, a man whose passion for magnets was matched only by his complete lack of social skills. He arrived moments later, his hair standing on end as if he'd just had a close encounter with a Van de Graaff generator.
+
+"Henderson," Arthur said, pointing to the key. "I need a way to attach this to a moving vehicle, from a distance, without anyone noticing."
+
+Henderson's eyes lit up. He pulled a small, powerful magnet from his pocket and held it near the key. The silver key, however, remained stubbornly non-magnetic. "Silver," Henderson muttered. "Tricky. But not impossible. We'll need a ferrous casing. And a magnetic delivery system. I'm thinking a modified crossbow."
+
+Montague stared at him. "A crossbow?"
+
+"Perfectly silent," Henderson said enthusiastically. "And with the right trajectory, we can attach the key to the underside of the Chancellor's car as it passes."
+
+Arthur, caught up in the moment, nodded. "Excellent. Montague, you'll need to get us the Chancellor's route. And a vantage point. Agnes, I want you to start a rumor that a rare species of migratory bird has been spotted in the vicinity of the German embassy. It will explain any suspicious-looking figures with binoculars."
+
+For the next twenty-four hours, the Department of Misplaced Car Keys, that forgotten corner of Whitehall, became the unlikely nerve center of a top-secret international operation. Henderson tinkered with his crossbow, Agnes spread rumors of a fictitious "Blue-Crested Warbler," and Arthur and Montague pored over maps of London, planning the perfect moment to return the lost key. The fate of Anglo-German relations, it seemed, now rested on a crossbow, a magnet, and the steady hand of Arthur Penhaligon.
+
+
+
+
+The chosen location was a narrow street near St. James's Park, a pinch point in the Chancellor's route to Buckingham Palace. Montague had secured a third-floor room in an empty office building overlooking the road. From the window, they had a clear, if slightly oblique, view of the street below.
+
+Henderson arrived with his creation in a violin case. The crossbow was a sleek, modern affair, matte black and silent. The bolt was a carbon-fiber rod with a small, powerful electromagnet at its tip, powered by a battery pack Henderson wore in a specially designed waistcoat. The key itself was nestled within a tiny, 3D-printed ferrous clamp, designed to attach to the bolt and then, with any luck, to the undercarriage of the Chancellor's limousine.
+
+"It's a work of art, Henderson," Arthur said, patting the eccentric inventor on the back.
+
+"The magnetic field is localized to a radius of two centimeters," Henderson explained proudly. "It will snap onto the chassis without so much as a clink. They'll never know it's there until the chauffeur finds it during his pre-journey inspection back in Berlin."
+
+Montague, dressed as a workman in a high-vis jacket, paced by the window, speaking into his cufflink. "Motorcade is five minutes out. The birdwatchers are in position."
+
+Arthur looked out of the window. On the opposite side of the street, he could see Agnes, disguised in a tweed skirt and a truly atrocious hat, peering intently into a tree with a pair of oversized binoculars. She was accompanied by two other DMCK employees, similarly attired. They looked, he had to admit, exactly like the kind of people who would get excited about a Blue-Crested Warbler.
+
+"It's all down to you now, Penhaligon," Montague said, his voice tense. "Are you steady?"
+
+Arthur, to his own surprise, was perfectly calm. He had spent his life dealing with the consequences of misplacement. This was simply the other side of the coin: a controlled, deliberate, and entirely necessary act of *placement*. He picked up the crossbow. It was lighter than he expected.
+
+"Target vehicle is the third in the convoy," Montague murmured. "Black Mercedes-Benz S-Class. You'll have a three-second window as it passes the lamppost."
+
+The distant wail of sirens grew louder. The street below was cleared of traffic by police outriders. Then, the convoy appeared. Two police cars, followed by the gleaming black limousine, flanked by two more cars carrying German security.
+
+Arthur rested the crossbow on the window ledge, his eye fixed on the approaching limousine. The world seemed to slow down. He could see the flag of the German Chancellor fluttering on the bonnet. He could see the impassive face of the driver. He could even see, he imagined, the faint outline of the Chancellor himself in the back seat, no doubt blissfully unaware of the drama unfolding just a few feet away.
+
+"Now, Penhaligon, now!" Montague hissed.
+
+Arthur squeezed the trigger. The bolt flew, a silent black streak against the grey London sky. For a heart-stopping moment, it seemed to be on a perfect trajectory. Then, a gust of wind, a freakish, unpredictable eddy that seemed to come from nowhere, caught the bolt. It veered, just slightly, to the left. Instead of attaching to the chassis, it struck the rear tire of the limousine with a soft, almost inaudible *thwump*.
+
+The key, dislodged from its clamp, flew off at a tangent, bounced once on the tarmac, and then, with a final, defiant gleam of silver, disappeared down a storm drain.
+
+For a full ten seconds, the room was silent, save for the receding wail of the sirens. Montague stared, aghast, at the street below. Henderson looked as if he was about to burst into tears.
+
+Arthur Penhaligon, the man who had built his career on order and precision, lowered the crossbow and uttered a single, uncharacteristic word.
+
+"Bother."
+
+
+
+
+Montague was apoplectic. He paced the room, hissing into his cufflink again. "The asset is compromised. I repeat, the asset is in a subterranean, aqueous environment. Abort! Abort!"
+
+Henderson was on his knees, head in his hands, muttering about wind shear and barometric pressure. "I didn't account for the microclimate of the street canyon! It's a classic rookie error!"
+
+But Arthur Penhaligon was already moving. The initial shock had passed, replaced by a steely resolve. This was no longer a simple matter of placement; it was a recovery operation. He strode over to the window and looked down at the storm drain, a small, unassuming grille in the pavement. It was just another kind of misplacement, he told himself. A slightly more… damp one.
+
+"Montague, pull yourself together," he said, his voice ringing with authority. "Panicking is not a strategy. Henderson, stop whimpering and calculate the likely water flow and trajectory of a silver key weighing approximately 47 grams in a Victorian-era drainage system."
+
+Henderson looked up, a flicker of his usual obsessive energy returning. "I'll need the schematics for the sewer network under St. James's."
+
+"Precisely," Arthur said. He turned to Montague. "Get me the head of the Department of Subterranean Maintenance. A man named Silas 'Digger' Jones. Tell him it's a matter of national importance. Code word: 'Warbler'."
+
+Montague stared at him. "'Digger' Jones? You're entrusting this to someone called 'Digger'?"
+
+"Silas Jones," Arthur said coolly, "knows more about what lies beneath London than any man alive. His department is responsible for everything from Roman-era aqueducts to Cold War-era secret tunnels. If that key is down there, he'll find it."
+
+Two hours later, in a damp, brick-lined tunnel beneath the streets of London, Arthur, Montague, and Henderson stood before a man who looked as if he had been carved from the very clay of the city. Silas 'Digger' Jones was short, immensely broad, and wore a set of waterproof overalls with the kind of pride a king might wear his coronation robes. He held a battered-looking tablet showing a complex map of pipes and tunnels.
+
+"Right then," Digger said, his voice a low rumble that seemed to echo in the subterranean quiet. "Based on your man's calculations of flow rate and the current water level, the object in question should have come to rest..." he tapped the screen, "...right here. In the main collector for this sector. Just before it hits the 'Tyburn Grate'."
+
+"And what, pray tell, is the 'Tyburn Grate'?" Montague asked, looking nervously at a trickle of murky water flowing past his immaculate shoes.
+
+"That's where we catch all the big stuff," Digger said cheerfully. "Anything that gets flushed that shouldn't. You wouldn't *believe* what we find down here. We once found a whole motorbike. Anyway, the key will be caught in the grille. We just need to go and get it."
+
+He handed them each a pair of thigh-high waders and a powerful headlamp. "It's about a quarter-mile walk. Mind your step, the brickwork is a bit slick in places. And whatever you do, don't touch the sides."
+
+And so, the unlikely team—the fastidious bureaucrat, the suave intelligence agent, and the obsessive scientist—donned their waders and followed the sewer king into the dark, gurgling depths beneath London, in search of a key that held the fate of nations in its silver teeth.
+
+
+
+
+The journey through the sewer was an assault on the senses. The air was thick with a smell that defied easy description, a pungent cocktail of damp earth, decay, and a century of London's secrets. The only sounds were the gurgle of the water, the drip-drip-drip from unseen pipes, and the squelch of their boots in the silt.
+
+Montague, for all his sangfroid, was clearly out of his element. He held a perfumed handkerchief to his nose and moved with a kind of gingerly horror, as if expecting a kraken to emerge from the murky depths at any moment. Henderson, on the other hand, was fascinated. He kept shining his headlamp on interesting fungal growths and muttering about anaerobic bacteria.
+
+Arthur, however, was focused. His world had been reduced to the narrow beam of his headlamp and the gruff guidance of Digger Jones. This was just another filing system, he reasoned, albeit a particularly damp and fragrant one. The principles were the same: a logical structure, a clear objective, and the unwavering belief that everything, eventually, could be found and put in its proper place.
+
+"Here we are," Digger announced, his voice echoing off the curved brick walls. "The Tyburn Grate."
+
+It was an impressive and horrifying sight. A massive iron grille, stretching from floor to ceiling, blocked the tunnel. The water flowed through it, but a vast, tangled mass of debris was caught on its bars. It was a museum of London's detritus: plastic bags, broken umbrellas, a traffic cone, a single, forlorn Wellington boot, and a truly astonishing number of tennis balls.
+
+"The key will be in there somewhere," Digger said, pointing a thumb at the mess. "Like finding a needle in a haystack. A very smelly haystack."
+
+He handed them each a long-handled rake. "Get comfy, lads. This could take a while."
+
+For the next hour, they raked through the debris, pulling aside clumps of sodden leaves and unidentifiable sludge. Montague worked with a look of profound self-loathing. Henderson, surprisingly, proved adept at the task, using his rake with a methodical precision. Arthur, meanwhile, coordinated the effort, dividing the grate into a grid and ensuring each section was searched systematically.
+
+"I've got something!" Henderson yelped, his voice high with excitement. He carefully maneuvered his rake and pulled a small, metallic object from the morass. It was a silver locket, tarnished and caked in grime.
+
+"Not it," Arthur said, without looking up. "Keep searching."
+
+Minutes later, Montague let out a muffled exclamation. "Is this it?" He held up a small, vaguely key-shaped piece of metal. It turned out to be a promotional bottle opener for a brand of German lager. The irony was not lost on them.
+
+Just as morale was beginning to ebb, Arthur spotted it. A faint glint of silver, caught between the bars of the grate, just at the water line. It was lodged in the spokes of a rusty bicycle wheel.
+
+"There!" he shouted, pointing with his rake. "There it is!"
+
+The key was tantalizingly out of reach, wedged tight. No amount of prodding with the rakes could dislodge it. Digger Jones waded forward and squinted at it. "She's stuck fast, alright. Only one way to get it."
+
+He looked at Arthur, a gleam in his eye. "One of us is going to have to reach in and get it."
+
+Before Montague could protest or Henderson could start calculating the probability of contracting a rare tropical disease, Arthur handed his rake to Digger. He rolled up the sleeve of his tweed jacket, took a deep breath, and plunged his arm into the cold, foul-smelling water and the tangled mess of the Tyburn Grate.
+
+His fingers closed around the cold, unmistakable shape of the Chancellor's Key. He pulled. It didn't move. He wiggled it, gently at first, then with more force. Finally, with a small sucking sound, it came free.
+
+Arthur Penhaligon stood up, dripping and filthy, but triumphant. In his hand, he held the key. It was covered in slime and smelled appalling, but it was intact. He had recovered the asset.
+
+"Right," he said, his voice dripping with sewer water and satisfaction. "Let's get out of here. We have a Chancellor to welcome."
+
+
+
+
+Back on the surface, blinking in the sudden daylight, the four men looked like creatures from another world. Montague, his suit ruined, his hair matted with grime, looked utterly defeated. Henderson was happily examining a sample of sewer slime in a test tube he had produced from his waistcoat. Digger Jones looked exactly the same as before.
+
+And Arthur Penhaligon, though he smelled faintly of the abyss, stood taller than ever. He held the key, now wiped clean on a handkerchief Montague had sacrificed with a small sob.
+
+"There is no time for a debrief," Arthur announced, taking command. "The Chancellor will be at his hotel. Montague, I need a car, a dry-cleaner who can perform miracles in under an hour, and access to the German delegation's security chief."
+
+Montague, galvanized by Arthur's decisiveness, sprang into action. Within minutes, a black car with tinted windows appeared. Within the hour, Arthur Penhaligon, now showered and immaculate in a freshly pressed suit, stood in the lobby of Claridge's hotel. He held a small, elegant box, sourced by one of Montague's innumerable contacts.
+
+"This is madness, Penhaligon," Montague whispered, hovering nervously by a potted palm. "You can't just walk up to them. Their head of security is a man named Richter. They call him 'The Wall'."
+
+"Walls have doors, Montague," Arthur said serenely. He had faced down the Phantom Key Jangler of Fleet Street. He was not about to be intimidated by a man named after a piece of masonry.
+
+He approached the formidable-looking man standing guard by the entrance to the Royal Suite. "Herr Richter?"
+
+The man turned, his eyes like chips of ice. "Who is asking?"
+
+"Arthur Penhaligon," he said, with a slight, formal bow. "Chairman of the Anglo-Germanic Automotive Livery Committee."
+
+Richter's expression did not change, but a flicker of confusion entered his eyes. "I have not heard of this committee."
+
+"We are a new initiative, established to foster closer ties through the celebration of our shared vehicular heritage," Arthur said smoothly, as if he were reciting from a well-rehearsed speech. "As a gesture of goodwill, and to commemorate the Chancellor's visit, we wished to present him with this small token."
+
+He opened the box. Inside, on a bed of fresh velvet, lay the Chancellor's key.
+
+Richter stared at the key. He looked at Arthur. He looked back at the key. A dozen emotions flickered across his face: suspicion, disbelief, and then, finally, a dawning, monumental relief.
+
+"Our Berlin office was supposed to have informed you of the presentation ceremony," Arthur continued, pressing his advantage. "There appears to have been an administrative oversight. A misplaced memo, perhaps. My department will be looking into it."
+
+He was giving Richter a way out, a plausible, bureaucratic explanation for a catastrophic security failure. A misplaced key was a disaster. A misplaced memo was just another day at the office.
+
+Richter took the box. He cleared his throat. "Yes," he said, his voice a low rumble. "The memo. I believe it arrived this morning. My apologies. There has been… much to occupy our attention."
+
+He gave a stiff, almost imperceptible nod. "Thank you, Herr Penhaligon. Your… committee… is very gracious."
+
+As Arthur turned and walked away, Montague fell into step beside him, his face a mask of astonishment. "The Anglo-Germanic Automotive Livery Committee?"
+
+"A department of one," Arthur said with a faint smile. "And now, I believe, officially disbanded. Its work is done."
+
+Back at the Department of Misplaced Car Keys, Agnes had brewed a pot of tea. The incident was over. The newspapers would report on a successful trade talk, on the seamless procession of the Chancellor's motorcade, on the deep and abiding friendship between the two nations. They would never know how close it had all come to being undone by a single, misplaced key.
+
+Arthur sat at his desk, the mahogany gleaming, the pens in their pot. It was once again an island of perfect order.
+
+"Anything to report, Agnes?" he asked.
+
+"A set of bicycle keys was found in the House of Commons library, sir," she said. "And a garage door opener has appeared in the Royal Opera House."
+
+"Standard stuff," Arthur murmured, making a note. The world, it seemed, was back to normal. And in the quiet, forgotten corners of Whitehall, the Department of Misplaced Car Keys continued its silent, unending, and profoundly important work.
+```
diff --git a/Applications/LLMEval/Models/LongPrompt.md b/Applications/LLMEval/Models/LongPrompt.md
new file mode 100644
index 0000000..85908ea
--- /dev/null
+++ b/Applications/LLMEval/Models/LongPrompt.md
@@ -0,0 +1,1348 @@
+Build me a command line chat tool that uses MLX and Swift. More details below... 
+
+# MLX SWIFT FRAMEWORK - COMPREHENSIVE DOCUMENTATION AND API REFERENCE
+
+This document provides complete documentation for MLX Swift, a Swift API for Apple's MLX machine learning framework. Use this documentation to understand how to build high-performance machine learning applications on Apple Silicon.
+
+## TABLE OF CONTENTS
+
+1. Introduction to MLX Swift
+2. Core Concepts and Architecture
+3. MLXArray - The Fundamental Data Structure
+4. Operations and Transformations
+5. Neural Network Module (MLX.NN)
+6. Language Model Support
+7. Memory Management and Performance
+8. Complete API Reference
+9. Code Examples
+
+---
+
+## 1. INTRODUCTION TO MLX SWIFT
+
+MLX Swift is the Swift API for MLX, a machine learning framework designed specifically for Apple Silicon. It provides:
+
+- **Lazy evaluation**: Operations are compiled and executed only when needed
+- **Unified memory**: Leverages Apple Silicon's unified memory architecture
+- **Multi-device support**: Seamlessly uses CPU and GPU
+- **Familiar NumPy-like API**: Easy to learn for those familiar with NumPy or PyTorch
+- **Composable function transformations**: Including automatic differentiation
+- **Dynamic graph construction**: No need to define static computation graphs
+
+MLX Swift brings all these capabilities to Swift, enabling native iOS and macOS applications with on-device machine learning.
+
+### Key Benefits
+
+- **Performance**: Optimized for Apple Silicon M-series chips
+- **Memory Efficiency**: Uses unified memory, avoiding costly data transfers
+- **Swift Integration**: First-class Swift API with type safety and modern Swift features
+- **On-Device ML**: Run large language models and other ML workloads locally
+- **SwiftUI Compatible**: Easy integration with SwiftUI for building user interfaces
+
+---
+
+## 2. CORE CONCEPTS AND ARCHITECTURE
+
+### Lazy Evaluation
+
+MLX uses lazy evaluation to optimize performance. Operations on arrays are not executed immediately; instead, they're recorded in a computation graph. The graph is compiled and executed only when the results are needed (e.g., when you call `eval()`).
+
+```swift
+let a = MLXArray(0 ..< 100)
+let b = a * 2  // Not yet computed
+let c = b + 10  // Still not computed
+let result = c.eval()  // Now the entire graph is compiled and executed
+```
+
+### Arrays and Devices
+
+MLX arrays can exist on different devices (CPU or GPU). Operations automatically handle device placement, but you can explicitly control this:
+
+```swift
+// Create array on GPU
+let gpuArray = MLXArray([1, 2, 3], device: .gpu)
+
+// Create array on CPU
+let cpuArray = MLXArray([4, 5, 6], device: .cpu)
+```
+
+### Broadcasting
+
+Like NumPy, MLX supports broadcasting - automatic expansion of arrays with different shapes during arithmetic operations:
+
+```swift
+let a = MLXArray([1, 2, 3])  // shape [3]
+let b = MLXArray([[1], [2], [3]])  // shape [3, 1]
+let c = a + b  // Result has shape [3, 3] via broadcasting
+```
+
+---
+
+## 3. MLXARRAY - THE FUNDAMENTAL DATA STRUCTURE
+
+`MLXArray` is the core data structure in MLX Swift, similar to NumPy's ndarray or PyTorch's Tensor.
+
+### Creating MLXArray
+
+```swift
+// From Swift arrays
+let arr1 = MLXArray([1, 2, 3, 4])
+
+// From ranges
+let arr2 = MLXArray(0 ..< 100)
+
+// With specific shape
+let arr3 = MLXArray(0 ..< 12, [3, 4])  // 3x4 matrix
+
+// Zeros, ones, and other constructors
+let zeros = MLXArray.zeros([10, 10])
+let ones = MLXArray.ones([5, 5])
+let random = MLXArray.random(0 ..< 1, [100, 100])
+
+// From scalar
+let scalar = MLXArray(3.14)
+
+// With specific dtype
+let floats = MLXArray([1, 2, 3], dtype: .float32)
+let ints = MLXArray([1.5, 2.7, 3.9], dtype: .int32)
+```
+
+### Array Properties
+
+```swift
+let arr = MLXArray(0 ..< 24, [2, 3, 4])
+
+arr.shape       // [2, 3, 4]
+arr.ndim        // 3
+arr.size        // 24
+arr.dtype       // .int32 or .float32, etc.
+arr.device      // .cpu or .gpu
+```
+
+### Indexing and Slicing
+
+```swift
+let matrix = MLXArray(0 ..< 20, [4, 5])
+
+// Single element
+let element = matrix[2, 3]
+
+// Slicing rows
+let firstRow = matrix[0]
+let lastRow = matrix[-1]
+
+// Slicing columns
+let firstCol = matrix[0..., 0]
+
+// Range slicing
+let subMatrix = matrix[1..<3, 2..<4]
+
+// Strided slicing
+let everyOther = matrix[0..., .stride(by: 2)]
+```
+
+### Reshaping and Transposing
+
+```swift
+let arr = MLXArray(0 ..< 12)
+
+// Reshape
+let reshaped = arr.reshaped([3, 4])
+let flattened = reshaped.flattened()
+
+// Transpose
+let matrix = MLXArray(0 ..< 6, [2, 3])
+let transposed = matrix.T
+let permuted = matrix.transposed(axes: [1, 0])
+
+// Squeeze and expand dims
+let squeezed = matrix.squeezed()
+let expanded = matrix.expandedDimensions(axis: 0)
+```
+
+---
+
+## 4. OPERATIONS AND TRANSFORMATIONS
+
+### Arithmetic Operations
+
+```swift
+let a = MLXArray([1, 2, 3, 4])
+let b = MLXArray([5, 6, 7, 8])
+
+// Element-wise operations
+let sum = a + b
+let diff = a - b
+let product = a * b
+let quotient = a / b
+let power = pow(a, b)
+let sqrt = sqrt(a)
+
+// Scalar operations
+let scaled = a * 2.0
+let offset = a + 10
+
+// In-place operations
+var c = a
+c += b
+c *= 2
+```
+
+### Mathematical Functions
+
+```swift
+let x = MLXArray([0, 0.5, 1.0, 1.5, 2.0])
+
+// Trigonometric
+let sinValues = sin(x)
+let cosValues = cos(x)
+let tanValues = tan(x)
+
+// Exponential and logarithmic
+let expValues = exp(x)
+let logValues = log(x + 1)  // Avoid log(0)
+let log10Values = log10(x + 1)
+
+// Rounding
+let rounded = round(x)
+let ceiled = ceil(x)
+let floored = floor(x)
+
+// Absolute value and sign
+let abs = abs(x)
+let sign = sign(x)
+
+// Clipping
+let clipped = clip(x, min: 0.5, max: 1.5)
+```
+
+### Reduction Operations
+
+```swift
+let matrix = MLXArray(1 ... 12, [3, 4])
+
+// Sum
+let totalSum = matrix.sum()  // Sum all elements
+let rowSum = matrix.sum(axis: 1)  // Sum along rows
+let colSum = matrix.sum(axis: 0)  // Sum along columns
+
+// Mean
+let mean = matrix.mean()
+let rowMean = matrix.mean(axis: 1)
+
+// Min and Max
+let minimum = matrix.min()
+let maximum = matrix.max()
+let argmin = matrix.argMin()  // Index of minimum
+let argmax = matrix.argMax()  // Index of maximum
+
+// Standard deviation and variance
+let std = matrix.std()
+let variance = matrix.variance()
+
+// Product
+let prod = matrix.product()
+
+// All and Any (for boolean arrays)
+let condition = matrix > 5
+let any = condition.any()
+let all = condition.all()
+```
+
+### Linear Algebra
+
+```swift
+// Matrix multiplication
+let A = MLXArray.random(0 ..< 1, [3, 4])
+let B = MLXArray.random(0 ..< 1, [4, 5])
+let C = matmul(A, B)  // Result is [3, 5]
+
+// Dot product (for vectors)
+let v1 = MLXArray([1, 2, 3])
+let v2 = MLXArray([4, 5, 6])
+let dotProduct = dot(v1, v2)
+
+// Outer product
+let outer = outerProduct(v1, v2)
+
+// Batched matrix multiplication
+let batch1 = MLXArray.random(0 ..< 1, [10, 3, 4])
+let batch2 = MLXArray.random(0 ..< 1, [10, 4, 5])
+let batchResult = matmul(batch1, batch2)  // [10, 3, 5]
+```
+
+### Comparison and Logical Operations
+
+```swift
+let a = MLXArray([1, 2, 3, 4, 5])
+let b = MLXArray([3, 3, 3, 3, 3])
+
+// Comparison operators
+let equal = a == b
+let notEqual = a != b
+let less = a < b
+let lessOrEqual = a <= b
+let greater = a > b
+let greaterOrEqual = a >= b
+
+// Logical operations
+let and = equal && notEqual
+let or = equal || notEqual
+let not = !equal
+
+// Where (conditional selection)
+let selected = MLXArray.where(a > 3, a, b)  // Select from a or b based on condition
+```
+
+### Concatenation and Stacking
+
+```swift
+let a = MLXArray([1, 2, 3])
+let b = MLXArray([4, 5, 6])
+
+// Concatenate
+let concatenated = MLXArray.concatenated([a, b], axis: 0)  // [1, 2, 3, 4, 5, 6]
+
+// Stack
+let stacked = MLXArray.stacked([a, b], axis: 0)  // [[1, 2, 3], [4, 5, 6]]
+
+// Split
+let parts = concatenated.split(indices: [3], axis: 0)  // Split at index 3
+```
+
+---
+
+## 5. NEURAL NETWORK MODULE (MLX.NN)
+
+MLX.NN provides building blocks for neural networks, similar to PyTorch's nn.Module.
+
+### Module Base Class
+
+All neural network layers inherit from `Module`:
+
+```swift
+class Module {
+    // Parameters are stored as MLXArray
+    func callAsFunction(_ inputs: MLXArray) -> MLXArray {
+        // Forward pass implementation
+        fatalError("Must override")
+    }
+    
+    // Get all trainable parameters
+    func parameters() -> [String: MLXArray] {
+        // Returns dictionary of parameter name -> array
+    }
+    
+    // Update parameters
+    func update(parameters: [String: MLXArray]) {
+        // Update module parameters
+    }
+    
+    // Training mode
+    var training: Bool = true
+}
+```
+
+### Common Layers
+
+#### Linear Layer
+
+```swift
+class Linear: Module {
+    let weight: MLXArray
+    let bias: MLXArray?
+    
+    init(inputDims: Int, outputDims: Int, bias: Bool = true) {
+        // Initialize weights and biases
+        self.weight = MLXArray.random(-1 ..< 1, [outputDims, inputDims])
+        if bias {
+            self.bias = MLXArray.zeros([outputDims])
+        } else {
+            self.bias = nil
+        }
+    }
+    
+    override func callAsFunction(_ x: MLXArray) -> MLXArray {
+        var output = matmul(x, weight.T)
+        if let bias = bias {
+            output = output + bias
+        }
+        return output
+    }
+}
+
+// Usage
+let linear = Linear(inputDims: 128, outputDims: 64)
+let input = MLXArray.random(0 ..< 1, [32, 128])  // Batch of 32
+let output = linear(input)  // [32, 64]
+```
+
+#### Embedding Layer
+
+```swift
+class Embedding: Module {
+    let weight: MLXArray
+    
+    init(vocabularySize: Int, dimensions: Int) {
+        self.weight = MLXArray.random(-0.1 ..< 0.1, [vocabularySize, dimensions])
+    }
+    
+    override func callAsFunction(_ indices: MLXArray) -> MLXArray {
+        // Look up embeddings for given indices
+        return weight[indices]
+    }
+}
+
+// Usage
+let embedding = Embedding(vocabularySize: 10000, dimensions: 256)
+let tokens = MLXArray([1, 42, 100, 256])
+let embeddings = embedding(tokens)  // [4, 256]
+```
+
+#### Convolutional Layers
+
+```swift
+class Conv2d: Module {
+    let weight: MLXArray
+    let bias: MLXArray?
+    let stride: (Int, Int)
+    let padding: (Int, Int)
+    
+    init(
+        inChannels: Int,
+        outChannels: Int,
+        kernelSize: (Int, Int),
+        stride: (Int, Int) = (1, 1),
+        padding: (Int, Int) = (0, 0),
+        bias: Bool = true
+    ) {
+        self.weight = MLXArray.random(
+            -0.1 ..< 0.1,
+            [outChannels, inChannels, kernelSize.0, kernelSize.1]
+        )
+        if bias {
+            self.bias = MLXArray.zeros([outChannels])
+        } else {
+            self.bias = nil
+        }
+        self.stride = stride
+        self.padding = padding
+    }
+    
+    override func callAsFunction(_ x: MLXArray) -> MLXArray {
+        // Convolution operation
+        var output = conv2d(x, weight, stride: stride, padding: padding)
+        if let bias = bias {
+            output = output + bias.reshaped([1, -1, 1, 1])
+        }
+        return output
+    }
+}
+```
+
+#### Normalization Layers
+
+```swift
+class LayerNorm: Module {
+    let weight: MLXArray
+    let bias: MLXArray
+    let eps: Float
+    
+    init(dimensions: Int, eps: Float = 1e-5) {
+        self.weight = MLXArray.ones([dimensions])
+        self.bias = MLXArray.zeros([dimensions])
+        self.eps = eps
+    }
+    
+    override func callAsFunction(_ x: MLXArray) -> MLXArray {
+        let mean = x.mean(axis: -1, keepDims: true)
+        let variance = x.variance(axis: -1, keepDims: true)
+        let normalized = (x - mean) / sqrt(variance + eps)
+        return normalized * weight + bias
+    }
+}
+
+class RMSNorm: Module {
+    let weight: MLXArray
+    let eps: Float
+    
+    init(dimensions: Int, eps: Float = 1e-5) {
+        self.weight = MLXArray.ones([dimensions])
+        self.eps = eps
+    }
+    
+    override func callAsFunction(_ x: MLXArray) -> MLXArray {
+        let rms = sqrt(x.square().mean(axis: -1, keepDims: true) + eps)
+        return (x / rms) * weight
+    }
+}
+```
+
+#### Activation Functions
+
+```swift
+// ReLU
+func relu(_ x: MLXArray) -> MLXArray {
+    return maximum(x, 0)
+}
+
+// GELU
+func gelu(_ x: MLXArray) -> MLXArray {
+    return x * 0.5 * (1.0 + erf(x / sqrt(2.0)))
+}
+
+// SiLU (Swish)
+func silu(_ x: MLXArray) -> MLXArray {
+    return x * sigmoid(x)
+}
+
+// Sigmoid
+func sigmoid(_ x: MLXArray) -> MLXArray {
+    return 1.0 / (1.0 + exp(-x))
+}
+
+// Tanh
+func tanh(_ x: MLXArray) -> MLXArray {
+    return MLX.tanh(x)
+}
+
+// Softmax
+func softmax(_ x: MLXArray, axis: Int = -1) -> MLXArray {
+    let maxVal = x.max(axis: axis, keepDims: true)
+    let expX = exp(x - maxVal)
+    return expX / expX.sum(axis: axis, keepDims: true)
+}
+
+// Log Softmax
+func logSoftmax(_ x: MLXArray, axis: Int = -1) -> MLXArray {
+    let maxVal = x.max(axis: axis, keepDims: true)
+    let shifted = x - maxVal
+    return shifted - log(exp(shifted).sum(axis: axis, keepDims: true))
+}
+```
+
+#### Dropout
+
+```swift
+class Dropout: Module {
+    let probability: Float
+    
+    init(probability: Float = 0.5) {
+        self.probability = probability
+    }
+    
+    override func callAsFunction(_ x: MLXArray) -> MLXArray {
+        guard training else { return x }
+        
+        let mask = MLXArray.random(0 ..< 1, x.shape) > probability
+        return x * mask / (1 - probability)
+    }
+}
+```
+
+### Sequential Container
+
+```swift
+class Sequential: Module {
+    let layers: [Module]
+    
+    init(_ layers: Module...) {
+        self.layers = layers
+    }
+    
+    override func callAsFunction(_ x: MLXArray) -> MLXArray {
+        var output = x
+        for layer in layers {
+            output = layer(output)
+        }
+        return output
+    }
+}
+
+// Usage
+let model = Sequential(
+    Linear(inputDims: 784, outputDims: 256),
+    ReLU(),
+    Dropout(probability: 0.5),
+    Linear(inputDims: 256, outputDims: 128),
+    ReLU(),
+    Linear(inputDims: 128, outputDims: 10)
+)
+```
+
+---
+
+## 6. LANGUAGE MODEL SUPPORT
+
+MLX Swift provides specialized support for running large language models efficiently.
+
+### Transformer Components
+
+#### Multi-Head Attention
+
+```swift
+class MultiHeadAttention: Module {
+    let numHeads: Int
+    let headDim: Int
+    let scale: Float
+    
+    let queryProj: Linear
+    let keyProj: Linear
+    let valueProj: Linear
+    let outProj: Linear
+    
+    init(dimensions: Int, numHeads: Int) {
+        self.numHeads = numHeads
+        self.headDim = dimensions / numHeads
+        self.scale = 1.0 / sqrt(Float(headDim))
+        
+        self.queryProj = Linear(inputDims: dimensions, outputDims: dimensions)
+        self.keyProj = Linear(inputDims: dimensions, outputDims: dimensions)
+        self.valueProj = Linear(inputDims: dimensions, outputDims: dimensions)
+        self.outProj = Linear(inputDims: dimensions, outputDims: dimensions)
+    }
+    
+    override func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
+        let B = x.shape[0]  // Batch size
+        let L = x.shape[1]  // Sequence length
+        
+        // Project to Q, K, V
+        var queries = queryProj(x).reshaped([B, L, numHeads, headDim]).transposed(axes: [0, 2, 1, 3])
+        var keys = keyProj(x).reshaped([B, L, numHeads, headDim]).transposed(axes: [0, 2, 1, 3])
+        var values = valueProj(x).reshaped([B, L, numHeads, headDim]).transposed(axes: [0, 2, 1, 3])
+        
+        // Attention scores
+        var scores = matmul(queries, keys.transposed(axes: [0, 1, 3, 2])) * scale
+        
+        // Apply mask if provided
+        if let mask = mask {
+            scores = scores + mask
+        }
+        
+        // Softmax and apply to values
+        let attn = softmax(scores, axis: -1)
+        var output = matmul(attn, values)
+        
+        // Reshape and project
+        output = output.transposed(axes: [0, 2, 1, 3]).reshaped([B, L, -1])
+        return outProj(output)
+    }
+}
+```
+
+#### Transformer Block
+
+```swift
+class TransformerBlock: Module {
+    let attention: MultiHeadAttention
+    let norm1: LayerNorm
+    let norm2: LayerNorm
+    let mlp: Sequential
+    
+    init(dimensions: Int, numHeads: Int, mlpDimensions: Int) {
+        self.attention = MultiHeadAttention(dimensions: dimensions, numHeads: numHeads)
+        self.norm1 = LayerNorm(dimensions: dimensions)
+        self.norm2 = LayerNorm(dimensions: dimensions)
+        
+        self.mlp = Sequential(
+            Linear(inputDims: dimensions, outputDims: mlpDimensions),
+            GELU(),
+            Linear(inputDims: mlpDimensions, outputDims: dimensions)
+        )
+    }
+    
+    override func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil) -> MLXArray {
+        // Self-attention with residual
+        var h = x + attention(norm1(x), mask: mask)
+        
+        // MLP with residual
+        h = h + mlp(norm2(h))
+        
+        return h
+    }
+}
+```
+
+### KV Cache for Fast Generation
+
+```swift
+class KVCache {
+    var keys: MLXArray?
+    var values: MLXArray?
+    let maxLength: Int
+    
+    init(maxLength: Int = 2048) {
+        self.maxLength = maxLength
+    }
+    
+    func update(keys newKeys: MLXArray, values newValues: MLXArray) -> (MLXArray, MLXArray) {
+        if let existingKeys = keys, let existingValues = values {
+            // Concatenate with existing cache
+            keys = MLXArray.concatenated([existingKeys, newKeys], axis: 2)
+            values = MLXArray.concatenated([existingValues, newValues], axis: 2)
+        } else {
+            // Initialize cache
+            keys = newKeys
+            values = newValues
+        }
+        
+        return (keys!, values!)
+    }
+    
+    func reset() {
+        keys = nil
+        values = nil
+    }
+}
+```
+
+### Text Generation
+
+```swift
+func generateText(
+    model: Module,
+    tokenizer: Tokenizer,
+    prompt: String,
+    maxTokens: Int = 100,
+    temperature: Float = 0.7,
+    topP: Float = 0.9
+) -> String {
+    // Tokenize prompt
+    var tokens = tokenizer.encode(prompt)
+    
+    // Generate tokens
+    for _ in 0 ..< maxTokens {
+        // Get logits from model
+        let input = MLXArray(tokens)
+        let logits = model(input.reshaped([1, -1]))
+        
+        // Sample next token
+        let nextToken = sample(
+            logits: logits[0, -1],
+            temperature: temperature,
+            topP: topP
+        )
+        
+        tokens.append(nextToken)
+        
+        // Check for end of sequence
+        if nextToken == tokenizer.eosToken {
+            break
+        }
+    }
+    
+    return tokenizer.decode(tokens)
+}
+
+func sample(logits: MLXArray, temperature: Float, topP: Float) -> Int {
+    // Apply temperature
+    let scaledLogits = logits / temperature
+    
+    // Convert to probabilities
+    let probs = softmax(scaledLogits)
+    
+    // Top-p (nucleus) sampling
+    let sortedProbs = probs.sorted(descending: true)
+    let cumulativeProbs = sortedProbs.cumsum()
+    
+    let mask = cumulativeProbs <= topP
+    let filteredProbs = MLXArray.where(mask, sortedProbs, 0)
+    
+    // Sample from filtered distribution
+    let normalizedProbs = filteredProbs / filteredProbs.sum()
+    return categorical(normalizedProbs)
+}
+```
+
+### Streaming Generation
+
+```swift
+func streamingGenerate(
+    model: Module,
+    tokenizer: Tokenizer,
+    prompt: String,
+    maxTokens: Int = 100,
+    onToken: (String) -> Void
+) async {
+    var tokens = tokenizer.encode(prompt)
+    
+    for _ in 0 ..< maxTokens {
+        let input = MLXArray(tokens)
+        let logits = model(input.reshaped([1, -1]))
+        
+        let nextToken = sample(logits: logits[0, -1], temperature: 0.7, topP: 0.9)
+        tokens.append(nextToken)
+        
+        // Stream token to callback
+        let tokenText = tokenizer.decode([nextToken])
+        onToken(tokenText)
+        
+        if nextToken == tokenizer.eosToken {
+            break
+        }
+        
+        // Allow UI updates
+        await Task.yield()
+    }
+}
+```
+
+---
+
+## 7. MEMORY MANAGEMENT AND PERFORMANCE
+
+### Evaluation and Materialization
+
+MLX uses lazy evaluation. To force computation:
+
+```swift
+let arr = MLXArray(0 ..< 1000) * 2 + 5
+
+// Force evaluation
+arr.eval()
+
+// Or access data (implicitly evaluates)
+let item = arr.item()  // For scalars
+let data = arr.asArray([Int].self)  // Convert to Swift array
+```
+
+### Memory Efficiency
+
+```swift
+// Free memory from arrays no longer needed
+var largeArray: MLXArray? = MLXArray.random(0 ..< 1, [10000, 10000])
+// ... use array ...
+largeArray = nil  // Release memory
+
+// Explicit memory management
+MLX.clearCache()  // Clear MLX's internal cache
+```
+
+### Performance Monitoring
+
+```swift
+import Foundation
+
+func measurePerformance<T>(_ operation: () -> T) -> (result: T, duration: TimeInterval) {
+    let start = Date()
+    let result = operation()
+    let duration = Date().timeIntervalSince(start)
+    return (result, duration)
+}
+
+// Usage
+let (result, duration) = measurePerformance {
+    let arr = MLXArray.random(0 ..< 1, [1000, 1000])
+    let product = matmul(arr, arr)
+    return product.eval()
+}
+
+print("Operation took \(duration) seconds")
+```
+
+### Tokens Per Second Tracking
+
+```swift
+class PerformanceTracker {
+    private var startTime: Date?
+    private var tokenCount: Int = 0
+    
+    func start() {
+        startTime = Date()
+        tokenCount = 0
+    }
+    
+    func recordToken() {
+        tokenCount += 1
+    }
+    
+    func tokensPerSecond() -> Double {
+        guard let start = startTime else { return 0 }
+        let elapsed = Date().timeIntervalSince(start)
+        return Double(tokenCount) / elapsed
+    }
+    
+    func reset() {
+        startTime = nil
+        tokenCount = 0
+    }
+}
+```
+
+### Batching for Efficiency
+
+```swift
+// Process multiple inputs together
+let batchSize = 32
+let inputs = MLXArray.random(0 ..< 1, [batchSize, 128])
+
+// Single forward pass for entire batch
+let outputs = model(inputs)  // More efficient than processing one-by-one
+```
+
+---
+
+## 8. COMPLETE API REFERENCE
+
+### MLXArray Static Methods
+
+```swift
+// Creation methods
+MLXArray.zeros(_ shape: [Int], dtype: DType = .float32, device: Device = .default)
+MLXArray.ones(_ shape: [Int], dtype: DType = .float32, device: Device = .default)
+MLXArray.full(_ shape: [Int], value: Float, dtype: DType = .float32)
+MLXArray.arange(start: Int = 0, end: Int, step: Int = 1, dtype: DType = .int32)
+MLXArray.linspace(start: Float, end: Float, count: Int, dtype: DType = .float32)
+MLXArray.random(_ range: Range<Float>, _ shape: [Int], dtype: DType = .float32)
+MLXArray.randomNormal(mean: Float = 0, std: Float = 1, _ shape: [Int])
+MLXArray.eye(_ n: Int, m: Int? = nil, k: Int = 0, dtype: DType = .float32)
+
+// Concatenation and stacking
+MLXArray.concatenated(_ arrays: [MLXArray], axis: Int = 0)
+MLXArray.stacked(_ arrays: [MLXArray], axis: Int = 0)
+
+// Conditional
+MLXArray.where(_ condition: MLXArray, _ x: MLXArray, _ y: MLXArray)
+```
+
+### MLXArray Instance Methods
+
+```swift
+// Shape manipulation
+func reshaped(_ shape: [Int]) -> MLXArray
+func flattened() -> MLXArray
+func squeezed(axis: Int? = nil) -> MLXArray
+func expandedDimensions(axis: Int) -> MLXArray
+func transposed(axes: [Int]? = nil) -> MLXArray
+
+// Arithmetic
+func sum(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+func mean(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+func min(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+func max(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+func product(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+func cumsum(axis: Int = -1) -> MLXArray
+
+// Statistical
+func variance(axis: Int? = nil, keepDims: Bool = false, ddof: Int = 0) -> MLXArray
+func std(axis: Int? = nil, keepDims: Bool = false, ddof: Int = 0) -> MLXArray
+
+// Searching
+func argMin(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+func argMax(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+
+// Logic
+func any(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+func all(axis: Int? = nil, keepDims: Bool = false) -> MLXArray
+
+// Evaluation
+func eval() -> MLXArray
+func item() -> Scalar  // For single-element arrays
+func asArray<T>(_ type: T.Type) -> [T]  // Convert to Swift array
+
+// Type conversion
+func asType(_ dtype: DType) -> MLXArray
+```
+
+### Global Functions
+
+```swift
+// Math operations
+func sqrt(_ x: MLXArray) -> MLXArray
+func exp(_ x: MLXArray) -> MLXArray
+func log(_ x: MLXArray) -> MLXArray
+func log2(_ x: MLXArray) -> MLXArray
+func log10(_ x: MLXArray) -> MLXArray
+func sin(_ x: MLXArray) -> MLXArray
+func cos(_ x: MLXArray) -> MLXArray
+func tan(_ x: MLXArray) -> MLXArray
+func abs(_ x: MLXArray) -> MLXArray
+func sign(_ x: MLXArray) -> MLXArray
+func square(_ x: MLXArray) -> MLXArray
+func pow(_ x: MLXArray, _ y: MLXArray) -> MLXArray
+func minimum(_ x: MLXArray, _ y: MLXArray) -> MLXArray
+func maximum(_ x: MLXArray, _ y: MLXArray) -> MLXArray
+func clip(_ x: MLXArray, min: Float, max: Float) -> MLXArray
+
+// Linear algebra
+func matmul(_ a: MLXArray, _ b: MLXArray) -> MLXArray
+func dot(_ a: MLXArray, _ b: MLXArray) -> MLXArray
+func outerProduct(_ a: MLXArray, _ b: MLXArray) -> MLXArray
+
+// Neural network operations
+func conv2d(_ input: MLXArray, _ weight: MLXArray, stride: (Int, Int), padding: (Int, Int)) -> MLXArray
+func maxPool2d(_ input: MLXArray, kernelSize: (Int, Int), stride: (Int, Int)) -> MLXArray
+func softmax(_ x: MLXArray, axis: Int) -> MLXArray
+func logSoftmax(_ x: MLXArray, axis: Int) -> MLXArray
+func relu(_ x: MLXArray) -> MLXArray
+func gelu(_ x: MLXArray) -> MLXArray
+func silu(_ x: MLXArray) -> MLXArray
+func sigmoid(_ x: MLXArray) -> MLXArray
+func tanh(_ x: MLXArray) -> MLXArray
+```
+
+### Data Types
+
+```swift
+enum DType {
+    case bool
+    case uint8
+    case uint16
+    case uint32
+    case uint64
+    case int8
+    case int16
+    case int32
+    case int64
+    case float16
+    case float32
+    case bfloat16
+    case complex64
+}
+```
+
+### Devices
+
+```swift
+enum Device {
+    case cpu
+    case gpu
+    case `default`  // Let MLX choose
+    
+    static var current: Device { get }
+}
+```
+
+---
+
+## 9. CODE EXAMPLES
+
+### Example 1: Simple Neural Network
+
+```swift
+class SimpleClassifier: Module {
+    let layer1: Linear
+    let layer2: Linear
+    let layer3: Linear
+    let dropout: Dropout
+    
+    init(inputSize: Int, hiddenSize: Int, numClasses: Int) {
+        self.layer1 = Linear(inputDims: inputSize, outputDims: hiddenSize)
+        self.layer2 = Linear(inputDims: hiddenSize, outputDims: hiddenSize)
+        self.layer3 = Linear(inputDims: hiddenSize, outputDims: numClasses)
+        self.dropout = Dropout(probability: 0.5)
+    }
+    
+    override func callAsFunction(_ x: MLXArray) -> MLXArray {
+        var h = relu(layer1(x))
+        h = dropout(h)
+        h = relu(layer2(h))
+        h = dropout(h)
+        return layer3(h)
+    }
+}
+
+// Training
+let model = SimpleClassifier(inputSize: 784, hiddenSize: 256, numClasses: 10)
+let optimizer = SGD(learningRate: 0.01)
+
+for epoch in 0 ..< 10 {
+    for (batch, labels) in dataLoader {
+        // Forward pass
+        let logits = model(batch)
+        let loss = crossEntropyLoss(logits: logits, labels: labels)
+        
+        // Backward pass
+        let gradients = grad(loss, model.parameters())
+        
+        // Update weights
+        optimizer.update(model, gradients: gradients)
+    }
+}
+```
+
+### Example 2: Image Processing
+
+```swift
+func processImage(_ image: MLXArray) -> MLXArray {
+    // Input: [H, W, C] image
+    // Normalize to [-1, 1]
+    var processed = (image / 127.5) - 1.0
+    
+    // Apply Gaussian blur (simple box filter approximation)
+    let kernel = MLXArray.ones([3, 3, 1, 1]) / 9.0
+    processed = conv2d(
+        processed.expandedDimensions(axis: 0),
+        kernel,
+        stride: (1, 1),
+        padding: (1, 1)
+    )
+    
+    // Edge detection
+    let sobelX = MLXArray([
+        [-1, 0, 1],
+        [-2, 0, 2],
+        [-1, 0, 1]
+    ]).reshaped([3, 3, 1, 1])
+    
+    let edges = conv2d(processed, sobelX, stride: (1, 1), padding: (1, 1))
+    
+    return edges.squeezed(axis: 0)
+}
+```
+
+### Example 3: Text Embedding Similarity
+
+```swift
+func cosineSimilarity(_ a: MLXArray, _ b: MLXArray) -> Float {
+    let dotProduct = (a * b).sum()
+    let normA = sqrt((a * a).sum())
+    let normB = sqrt((b * b).sum())
+    return (dotProduct / (normA * normB)).item()
+}
+
+let embedding = Embedding(vocabularySize: 10000, dimensions: 512)
+
+let sentence1 = MLXArray([42, 123, 456, 789])
+let sentence2 = MLXArray([42, 125, 458, 791])
+
+let emb1 = embedding(sentence1).mean(axis: 0)
+let emb2 = embedding(sentence2).mean(axis: 0)
+
+let similarity = cosineSimilarity(emb1, emb2)
+print("Similarity: \(similarity)")
+```
+
+### Example 4: Attention Mechanism
+
+```swift
+func scaledDotProductAttention(
+    query: MLXArray,
+    key: MLXArray,
+    value: MLXArray,
+    mask: MLXArray? = nil
+) -> MLXArray {
+    let dk = Float(query.shape[-1])
+    var scores = matmul(query, key.transposed(axes: [0, 1, 3, 2])) / sqrt(dk)
+    
+    if let mask = mask {
+        scores = scores + mask
+    }
+    
+    let attention = softmax(scores, axis: -1)
+    return matmul(attention, value)
+}
+```
+
+### Example 5: LLM Inference with Performance Tracking
+
+```swift
+class LLMInference {
+    let model: Module
+    let tokenizer: Tokenizer
+    let performanceTracker = PerformanceTracker()
+    
+    init(model: Module, tokenizer: Tokenizer) {
+        self.model = model
+        self.tokenizer = tokenizer
+    }
+    
+    func generate(prompt: String, maxTokens: Int = 100) -> (text: String, tokensPerSecond: Double) {
+        performanceTracker.start()
+        
+        var tokens = tokenizer.encode(prompt)
+        var generatedText = prompt
+        
+        for _ in 0 ..< maxTokens {
+            let input = MLXArray(tokens).reshaped([1, -1])
+            let logits = model(input)
+            
+            let nextToken = sample(logits: logits[0, -1], temperature: 0.7, topP: 0.9)
+            tokens.append(nextToken)
+            
+            let tokenText = tokenizer.decode([nextToken])
+            generatedText += tokenText
+            
+            performanceTracker.recordToken()
+            
+            if nextToken == tokenizer.eosToken {
+                break
+            }
+        }
+        
+        let tps = performanceTracker.tokensPerSecond()
+        return (generatedText, tps)
+    }
+}
+```
+
+---
+
+# YOUR TASK: Build a Command-Line LLM Chat Tool
+
+  Using the MLX Swift documentation above, create a **single-file command-line Swift program** that demonstrates
+  core MLX concepts for LLM inference.
+
+  ## Requirements
+
+  Create a file called `mlx-chat.swift` that implements a simple but complete command-line chat interface with an
+  LLM.
+
+  ### 1. Model Loading
+
+  **Load a quantized model:**
+  - Use `loadModel(id:)` to load "mlx-community/Qwen2-0.5B-Instruct-4bit" (small, fast model)
+  - Show loading progress with simple text updates
+  - Handle loading errors gracefully
+
+  **In your comments, explain:**
+  - Why quantized models (4-bit) are used for on-device inference
+  - How lazy evaluation affects model loading
+  - The role of unified memory in model weight storage
+
+  ### 2. Chat Session with Streaming
+
+  **Implement a REPL (Read-Eval-Print Loop):**
+  - Use `ChatSession` for conversation management
+  - Stream responses token-by-token to stdout using `streamResponse(to:)`
+  - Support multi-turn conversations (context preservation)
+  - Allow user to type "exit" or "quit" to end
+  - Clear command ("clear") to reset conversation
+
+  **In your comments, explain:**
+  - How KV cache enables efficient multi-turn chat
+  - Why streaming is important for user experience
+  - The difference between eager and lazy evaluation during generation
+
+  ### 3. Performance Metrics
+
+  **Track and display after each response:**
+  - Tokens per second: calculate from generation time
+  - Time to first token (TTFT)
+  - Total tokens in response
+  - Total generation time
+  - GPU memory usage (use `GPU.snapshot()`)
+
+  **Print metrics in a clean format:**
+  ─────────────────────────────
+  📊 Metrics
+  ─────────────────────────────
+  ⚡ Speed: 42.5 tok/s
+  ⏱️  First token: 156ms
+  📝 Tokens: 85
+  ⏰ Total time: 2.0s
+  💾 GPU memory: 1.2 GB
+  ─────────────────────────────
+
+  **In your comments, explain:**
+  - How unified memory benefits performance monitoring
+  - Why GPU memory doesn't need to be copied to CPU for metrics
+  - The relationship between memory usage and KV cache size
+
+  ### 4. Generation Parameters
+
+  **Support command-line arguments:**
+  ```bash
+  swift mlx-chat.swift --temperature 0.7 --max-tokens 500
+
+  Parameters to support:
+  - --temperature (default: 0.6)
+  - --max-tokens (default: 200)
+  - --model (default: "mlx-community/Qwen2-0.5B-Instruct-4bit")
+
+  5. Code Structure
+
+  Single file structure (~200-300 lines):
+
+  import Foundation
+  import MLX
+  import MLXLLM
+  import MLXLMCommon
+
+  // MARK: - Configuration
+  struct ChatConfig {
+      let modelId: String
+      let temperature: Float
+      let maxTokens: Int
+
+      static func parseArguments() -> ChatConfig {
+          // Parse CommandLine.arguments
+      }
+  }
+
+  // MARK: - Metrics Tracking
+  struct GenerationMetrics {
+      var tokensPerSecond: Double
+      var timeToFirstToken: TimeInterval
+      var totalTokens: Int
+      var totalTime: TimeInterval
+      var gpuMemoryMB: Double
+
+      func display() {
+          // Pretty print metrics
+      }
+  }
+
+  // MARK: - Main Chat Loop
+  @main
+  struct MLXChat {
+      static func main() async throws {
+          // 1. Parse arguments
+          // 2. Load model with progress
+          // 3. Create ChatSession
+          // 4. Enter REPL loop
+          // 5. Handle streaming responses
+          // 6. Track and display metrics
+      }
+  }
+
+  // MARK: - Helper Functions
+  func printWelcome(config: ChatConfig) { }
+  func printMetrics(_ metrics: GenerationMetrics) { }
+  func readInput(prompt: String) -> String? { }
+
+  6. Detailed Comments Required
+
+  Add explanatory comments for these MLX-specific concepts:
+
+  Lazy Evaluation:
+  // MLX uses lazy evaluation - the model weights aren't actually loaded
+  // into memory until we call eval() or perform an operation that requires
+  // materialization. This allows efficient weight updates and transformations
+  // before any computation happens.
+  let model = try await loadModel(id: config.modelId)
+
+  KV Cache:
+  // ChatSession maintains a KV (key-value) cache across conversation turns.
+  // Instead of reprocessing all previous tokens on each turn, the cache
+  // stores attention keys and values, enabling O(1) context instead of O(n).
+  // Trade-off: memory usage grows linearly with conversation length.
+  let session = ChatSession(model, generateParameters: params)
+
+  Unified Memory:
+  // Apple Silicon's unified memory means GPU and CPU share the same physical
+  // memory. We can access model weights and activations from both CPU (for
+  // tokenization) and GPU (for inference) without copying. This is why
+  // GPU.snapshot() can report memory instantly without data transfers.
+  let snapshot = GPU.snapshot()
+  let memoryMB = Double(snapshot.activeMemory) / 1024 / 1024
+
+  Streaming Generation:
+  // Streaming allows displaying tokens as they're generated rather than
+  // waiting for the complete response. Each iteration evaluates just enough
+  // of the computation graph to produce the next token. This creates a
+  // responsive user experience.
+  for try await token in session.streamResponse(to: prompt) {
+      print(token, terminator: "")
+      fflush(stdout)  // Force immediate display
+  }
+
+  Example Session
+
+  $ swift mlx-chat.swift --temperature 0.7 --max-tokens 300
+
+  🤖 MLX Chat - Loading model...
+  📦 Model: mlx-community/Qwen2-0.5B-Instruct-4bit
+  ✅ Ready! Type 'exit' to quit, 'clear' to reset conversation.
+
+  You: What is Swift?
diff --git a/Applications/LLMEval/Models/PresetPrompts.swift b/Applications/LLMEval/Models/PresetPrompts.swift
new file mode 100644
index 0000000..fabf890
--- /dev/null
+++ b/Applications/LLMEval/Models/PresetPrompts.swift
@@ -0,0 +1,52 @@
+// Copyright © 2025 Apple Inc.
+
+import Foundation
+
+struct PresetPrompt: Identifiable {
+    let id = UUID()
+    let prompt: String
+    let enableTools: Bool
+    let enableThinking: Bool
+    let isLongPrompt: Bool
+
+    init(
+        _ prompt: String, enableTools: Bool = false, enableThinking: Bool = false,
+        isLongPrompt: Bool = false
+    ) {
+        self.prompt = prompt
+        self.enableTools = enableTools
+        self.enableThinking = enableThinking
+        self.isLongPrompt = isLongPrompt
+    }
+}
+
+struct PresetPrompts {
+    // Helper to load prompts from markdown files
+    private static func loadPrompt(named fileName: String) -> String {
+        guard let url = Bundle.main.url(forResource: fileName, withExtension: "md"),
+            let content = try? String(contentsOf: url, encoding: .utf8)
+        else {
+            return "Could not load \(fileName).md. Please ensure it is included in the app bundle."
+        }
+        return content
+    }
+
+    static let all: [PresetPrompt] = [
+        PresetPrompt("Why is the sky blue?"),
+        PresetPrompt("What would a medieval knight's Yelp review of a dragon's lair look like?"),
+        PresetPrompt("Explain why socks disappear in the dryer from the dryer's perspective."),
+
+        PresetPrompt(
+            "Write a breaking news report about cats discovering they can vote.",
+            enableThinking: true),
+        PresetPrompt(
+            "Write a performance review for the person whose job is to make sure Mondays feel terrible.",
+            enableThinking: true),
+
+        PresetPrompt("What's the weather in Paris?", enableTools: true),
+        PresetPrompt("What is the current time?", enableTools: true),
+
+        PresetPrompt(loadPrompt(named: "LongPrompt"), enableThinking: true, isLongPrompt: true),
+        PresetPrompt(loadPrompt(named: "CarKeysStory"), isLongPrompt: true),
+    ]
+}
diff --git a/Applications/LLMEval/Models/ToolDefinitions.swift b/Applications/LLMEval/Models/ToolDefinitions.swift
new file mode 100644
index 0000000..88be040
--- /dev/null
+++ b/Applications/LLMEval/Models/ToolDefinitions.swift
@@ -0,0 +1,34 @@
+// Copyright © 2025 Apple Inc.
+
+import Foundation
+
+// MARK: - Weather Tool
+
+struct WeatherInput: Codable {
+    let location: String
+    let unit: String?
+}
+
+struct WeatherOutput: Codable {
+    let temperature: Double
+    let conditions: String
+}
+
+// MARK: - Add Tool
+
+struct AddInput: Codable {
+    let first: Int
+    let second: Int
+}
+
+struct AddOutput: Codable {
+    let result: Int
+}
+
+// MARK: - Time Tool
+
+struct EmptyInput: Codable {}
+
+struct TimeOutput: Codable {
+    let time: String
+}
diff --git a/Applications/LLMEval/Preview Content/Preview Assets.xcassets/Contents.json b/Applications/LLMEval/Preview Content/Preview Assets.xcassets/Contents.json
deleted file mode 100644
index 73c0059..0000000
--- a/Applications/LLMEval/Preview Content/Preview Assets.xcassets/Contents.json	
+++ /dev/null
@@ -1,6 +0,0 @@
-{
-  "info" : {
-    "author" : "xcode",
-    "version" : 1
-  }
-}
diff --git a/Applications/LLMEval/README.md b/Applications/LLMEval/README.md
index 606fd64..c786750 100644
--- a/Applications/LLMEval/README.md
+++ b/Applications/LLMEval/README.md
@@ -2,7 +2,7 @@
 
 An example that:
 
-- downloads a huggingface model (phi-2) and tokenizer
+- downloads a huggingface model and tokenizer
 - evaluates a prompt
 - displays the output as it generates text
 
@@ -13,17 +13,13 @@ Some notes about the setup:
 - this downloads models from hugging face so LLMEval -> Signing & Capabilities has the "Outgoing Connections (Client)" set in the App Sandbox
 - LLM models are large so this uses the Increased Memory Limit entitlement on iOS to allow ... increased memory limits for devices that have more memory
 - `MLX.GPU.set(cacheLimit: 20 * 1024 * 1024)` is used to limit the buffer cache size
-- The Phi2 4 bit model is small enough to run on some iPhone models
-    - this can be changed by editing `let modelConfiguration = ModelConfiguration.phi4bit`
 
 ### Trying Different Models
 
-The example application uses Phi2 model by default, see [ContentView.swift](ContentView.swift#L58):
+The example app uses an 8 billion parameter quantized Qwen3 model by default, see [LLMEvaluator.swift](ViewModels/LLMEvaluator.swift#L52):
 
 ```
-    /// this controls which model loads -- phi4bit is one of the smaller ones so this will fit on
-    /// more devices
-    let modelConfiguration = ModelConfiguration.phi4bit
+    var modelConfiguration = LLMRegistry.qwen3_8b_4bit
 ```
 
 There are some pre-configured models in [MLXLLM/LLMModelFactory.swift](../../Libraries/MLXLLM/LLMModelFactory.swift#L78)
@@ -31,6 +27,12 @@ and you can load any weights from Hugging Face where there
 is a model architecture defined and you have enough
 memory.
 
+For example:
+```
+    /// phi4bit is one of the smaller models so will fit on more devices
+    var modelConfiguration = LLMRegistry.phi4bit
+```
+
 ### Troubleshooting
 
 If the program crashes with a very deep stack trace, you may need to build
diff --git a/Applications/LLMEval/Services/FormatUtilities.swift b/Applications/LLMEval/Services/FormatUtilities.swift
new file mode 100644
index 0000000..c0f74bb
--- /dev/null
+++ b/Applications/LLMEval/Services/FormatUtilities.swift
@@ -0,0 +1,26 @@
+// Copyright © 2025 Apple Inc.
+
+import Foundation
+
+/// Utility functions for formatting values
+enum FormatUtilities {
+
+    /// Formats a byte count into a human-readable string with appropriate units
+    /// - Parameter bytes: The number of bytes to format
+    /// - Returns: A formatted string (e.g., "2.5 GB", "128 MB", "512 KB")
+    static func formatMemory(_ bytes: Int) -> String {
+        let kb = Double(bytes) / 1024
+        let mb = kb / 1024
+        let gb = mb / 1024
+
+        if gb >= 1 {
+            return String(format: "%.2f GB", gb)
+        } else if mb >= 1 {
+            return String(format: "%.0f MB", mb)
+        } else if kb >= 1 {
+            return String(format: "%.0f KB", kb)
+        } else {
+            return "0 KB"
+        }
+    }
+}
diff --git a/Applications/LLMEval/Services/ToolExecutor.swift b/Applications/LLMEval/Services/ToolExecutor.swift
new file mode 100644
index 0000000..8d369ae
--- /dev/null
+++ b/Applications/LLMEval/Services/ToolExecutor.swift
@@ -0,0 +1,77 @@
+// Copyright © 2025 Apple Inc.
+
+import Foundation
+import MLXLMCommon
+
+public typealias ToolSpec = [String: Any]
+
+/// Manages tool definitions and execution for LLM function calling
+@MainActor
+class ToolExecutor {
+
+    // MARK: - Tool Definitions
+
+    let currentWeatherTool = Tool<WeatherInput, WeatherOutput>(
+        name: "get_current_weather",
+        description: "Get the current weather in a given location",
+        parameters: [
+            .required(
+                "location", type: .string, description: "The city and state, e.g. San Francisco, CA"
+            ),
+            .optional(
+                "unit",
+                type: .string,
+                description: "The unit of temperature",
+                extraProperties: [
+                    "enum": ["celsius", "fahrenheit"],
+                    "default": "celsius",
+                ]
+            ),
+        ]
+    ) { input in
+        let range = input.unit == "celsius" ? (min: -20.0, max: 40.0) : (min: 0, max: 100)
+        let temperature = Double.random(in: range.min ... range.max)
+        let conditions = ["Sunny", "Cloudy", "Rainy", "Snowy", "Windy", "Stormy"].randomElement()!
+        return WeatherOutput(temperature: temperature, conditions: conditions)
+    }
+
+    let addTool = Tool<AddInput, AddOutput>(
+        name: "add_two_numbers",
+        description: "Add two numbers together",
+        parameters: [
+            .required("first", type: .int, description: "The first number to add"),
+            .required("second", type: .int, description: "The second number to add"),
+        ]
+    ) { input in
+        AddOutput(result: input.first + input.second)
+    }
+
+    let timeTool = Tool<EmptyInput, TimeOutput>(
+        name: "get_time",
+        description: "Get the current time",
+        parameters: []
+    ) { _ in
+        TimeOutput(time: Date.now.formatted())
+    }
+
+    // MARK: - Tool Execution
+
+    /// Returns all available tool schemas
+    var allToolSchemas: [ToolSpec] {
+        [currentWeatherTool.schema, addTool.schema, timeTool.schema]
+    }
+
+    /// Executes a tool call and returns the result
+    func execute(_ toolCall: ToolCall) async throws -> String {
+        switch toolCall.function.name {
+        case currentWeatherTool.name:
+            return try await toolCall.execute(with: currentWeatherTool).toolResult
+        case addTool.name:
+            return try await toolCall.execute(with: addTool).toolResult
+        case timeTool.name:
+            return try await toolCall.execute(with: timeTool).toolResult
+        default:
+            return "Unknown tool: \(toolCall.function.name)"
+        }
+    }
+}
diff --git a/Applications/LLMEval/ViewModels/DeviceStat.swift b/Applications/LLMEval/ViewModels/DeviceStat.swift
index d677b7c..e47375b 100644
--- a/Applications/LLMEval/ViewModels/DeviceStat.swift
+++ b/Applications/LLMEval/ViewModels/DeviceStat.swift
@@ -1,3 +1,5 @@
+// Copyright © 2025 Apple Inc.
+
 import Foundation
 import MLX
 
diff --git a/Applications/LLMEval/ViewModels/LLMEvaluator.swift b/Applications/LLMEval/ViewModels/LLMEvaluator.swift
new file mode 100644
index 0000000..a624116
--- /dev/null
+++ b/Applications/LLMEval/ViewModels/LLMEvaluator.swift
@@ -0,0 +1,393 @@
+// Copyright © 2025 Apple Inc.
+
+import Hub
+import MLX
+import MLXLLM
+import MLXLMCommon
+import Metal
+import SwiftUI
+
+@Observable
+@MainActor
+class LLMEvaluator {
+
+    var running = false
+
+    var includeWeatherTool = false
+    var enableThinking = false
+    var maxTokens = 2048
+
+    var prompt = ""
+    var output = ""
+    var modelInfo = ""
+
+    // Download progress tracking
+    var downloadProgress: Double?
+    var totalSize: String?
+
+    // Performance metrics
+    var tokensPerSecond: Double = 0.0
+    var timeToFirstToken: Double = 0.0
+    var promptLength: Int = 0
+    var totalTokens: Int = 0
+    var totalTime: Double = 0.0
+
+    // Track if generation was truncated due to hitting max tokens
+    var wasTruncated: Bool = false
+
+    // Timer for tracking TTFT in real-time
+    private var ttftTimer: Timer?
+    private var generationStartTime: TimeInterval = 0
+
+    // Timer for tracking tokens/sec and total time in real-time
+    private var generationTimer: Timer?
+    private var firstTokenTime: TimeInterval = 0
+
+    /// This controls which model loads.
+    var modelConfiguration = LLMRegistry.qwen3_8b_4bit
+
+    /// Parameters controlling the generation output (max tokens and temperature).
+    var generateParameters: GenerateParameters {
+        GenerateParameters(maxTokens: maxTokens, temperature: 0.6)
+    }
+
+    /// A task responsible for handling the generation process.
+    var generationTask: Task<Void, Error>?
+
+    /// Tool executor for function calling
+    private let toolExecutor = ToolExecutor()
+
+    enum LoadState {
+        case idle
+        case loading
+        case loaded(ModelContainer)
+    }
+
+    var loadState = LoadState.idle
+
+    var isLoading: Bool {
+        if case .loading = loadState {
+            return true
+        }
+        return false
+    }
+
+    /// Short model name extracted from the full model ID.
+    private var modelName: String {
+        modelConfiguration.name.components(separatedBy: "/").last ?? modelConfiguration.name
+    }
+
+    /// Load and return the model. Can be called multiple times; subsequent calls return the cached model.
+    func load() async throws -> ModelContainer {
+        switch loadState {
+        case .idle:
+            return try await performLoad()
+
+        case .loading:
+            // Already loading, wait and retry
+            try await Task.sleep(nanoseconds: 100_000_000)  // 100ms
+            return try await load()
+
+        case .loaded(let modelContainer):
+            return modelContainer
+        }
+    }
+
+    private func performLoad() async throws -> ModelContainer {
+        loadState = .loading
+        modelInfo = "Downloading \(modelName)..."
+        downloadProgress = 0.0
+
+        MLX.GPU.set(cacheLimit: 20 * 1024 * 1024)
+
+        let hub = HubApi(
+            downloadBase: FileManager.default.urls(for: .cachesDirectory, in: .userDomainMask).first
+        )
+
+        do {
+            let modelDirectory = try await downloadModel(
+                hub: hub,
+                configuration: modelConfiguration
+            ) { [weak self] progress in
+                Task { @MainActor in
+                    self?.updateDownloadProgress(progress)
+                }
+            }
+
+            // Verify the download succeeded by checking for model files
+            let fileManager = FileManager.default
+            let directoryExists = fileManager.fileExists(atPath: modelDirectory.path)
+            let contents = (try? fileManager.contentsOfDirectory(atPath: modelDirectory.path)) ?? []
+            let hasSafetensors = contents.contains { $0.hasSuffix(".safetensors") }
+
+            if !directoryExists || !hasSafetensors {
+                throw NSError(
+                    domain: "LLMEvaluator",
+                    code: -1,
+                    userInfo: [
+                        NSLocalizedDescriptionKey:
+                            "Model download failed. Please check your network connection and try again."
+                    ]
+                )
+            }
+
+            modelInfo = "Loading \(modelName)..."
+            downloadProgress = nil
+            totalSize = nil
+
+            let modelContainer = try await LLMModelFactory.shared.loadContainer(
+                hub: hub,
+                configuration: modelConfiguration
+            ) { _ in }
+
+            let numParams = await modelContainer.perform { $0.model.numParameters() }
+
+            self.prompt = PresetPrompts.all[0].prompt
+            self.modelInfo = formatModelInfo(name: modelConfiguration.name, parameters: numParams)
+            loadState = .loaded(modelContainer)
+            return modelContainer
+
+        } catch {
+            resetLoadingState()
+            throw error
+        }
+    }
+
+    private func updateDownloadProgress(_ progress: Progress) {
+        modelInfo = "Downloading \(modelName) (\(Int(progress.fractionCompleted * 100))%)"
+        downloadProgress = progress.fractionCompleted
+
+        // Get file count info
+        if progress.totalUnitCount > 0 && progress.totalUnitCount < 100 {
+            totalSize = "File \(progress.completedUnitCount + 1) of \(progress.totalUnitCount)"
+        } else if progress.totalUnitCount > 0 {
+            totalSize =
+                "\(formatBytes(progress.completedUnitCount)) of \(formatBytes(progress.totalUnitCount))"
+        } else {
+            totalSize = nil
+        }
+    }
+
+    private func resetLoadingState() {
+        loadState = .idle
+        downloadProgress = nil
+        totalSize = nil
+    }
+
+    private func formatModelInfo(name: String, parameters: Int) -> String {
+        // Extract model name from full ID (e.g., "mlx-community/Qwen3-8B-4bit" -> "Qwen3-8B-4bit")
+        let modelName = name.components(separatedBy: "/").last ?? name
+
+        // Format parameter count (convert millions to billions if appropriate)
+        let paramMillions = parameters / (1024 * 1024)
+        let paramString: String
+        if paramMillions >= 1000 {
+            let paramBillions = Double(paramMillions) / 1000.0
+            paramString = String(format: "%.1fB", paramBillions)
+        } else {
+            paramString = "\(paramMillions)M"
+        }
+
+        return "\(modelName) • \(paramString) parameters"
+    }
+
+    private func formatBytes(_ bytes: Int64) -> String {
+        let formatter = ByteCountFormatter()
+        formatter.allowedUnits = [.useMB, .useGB]
+        formatter.countStyle = .file
+        return formatter.string(fromByteCount: bytes)
+    }
+
+    private func generate(prompt: String, toolResult: String? = nil) async {
+        // Only clear output if this is a fresh generation (not a tool continuation)
+        if toolResult == nil {
+            self.output = ""
+            // Reset metrics at start
+            self.totalTokens = 0
+            self.tokensPerSecond = 0.0
+            self.promptLength = 0
+            self.timeToFirstToken = 0.0
+            self.totalTime = 0.0
+            self.wasTruncated = false
+
+            // Start the real-time TTFT timer
+            generationStartTime = Date.timeIntervalSinceReferenceDate
+            ttftTimer?.invalidate()
+            ttftTimer = Timer.scheduledTimer(withTimeInterval: 0.01, repeats: true) {
+                [weak self] _ in
+                guard let self = self else { return }
+                Task { @MainActor in
+                    let elapsed = Date.timeIntervalSinceReferenceDate - self.generationStartTime
+                    self.timeToFirstToken = elapsed * 1000  // Convert to ms
+                }
+            }
+        }
+
+        var chat: [Chat.Message] = [
+            .system("You are a helpful assistant"),
+            .user(prompt),
+        ]
+
+        if let toolResult {
+            chat.append(.tool(toolResult))
+        }
+
+        let userInput = UserInput(
+            chat: chat,
+            tools: includeWeatherTool ? toolExecutor.allToolSchemas : nil,
+            additionalContext: ["enable_thinking": enableThinking]
+        )
+
+        do {
+            let modelContainer = try await load()
+
+            // Capture parameters on MainActor before entering perform block
+            let parameters = generateParameters
+
+            // Seed random generator to ensure varied output each generation
+            MLXRandom.seed(UInt64(Date.timeIntervalSinceReferenceDate * 1000))
+
+            try await modelContainer.perform { (context: ModelContext) -> Void in
+                let lmInput = try await context.processor.prepare(input: userInput)
+                let start = Date.timeIntervalSinceReferenceDate
+                let stream = try MLXLMCommon.generate(
+                    input: lmInput, parameters: parameters, context: context)
+
+                var iterator = stream.makeAsyncIterator()
+                if let first = await iterator.next() {
+                    let firstTick = Date.timeIntervalSinceReferenceDate
+                    let promptTime = firstTick - start
+                    let promptTokenCount = lmInput.text.tokens.size
+
+                    // Update TTFT and prompt length
+                    Task { @MainActor in
+                        self.ttftTimer?.invalidate()
+                        self.ttftTimer = nil
+                        self.timeToFirstToken = promptTime * 1000  // Convert to ms
+                        self.promptLength = promptTokenCount
+
+                        // Start real-time generation metrics tracking
+                        self.firstTokenTime = Date.timeIntervalSinceReferenceDate
+                        self.generationTimer?.invalidate()
+                        self.generationTimer = Timer.scheduledTimer(
+                            withTimeInterval: 0.1, repeats: true
+                        ) { [weak self] _ in
+                            guard let self = self else { return }
+                            Task { @MainActor in
+                                let elapsed =
+                                    Date.timeIntervalSinceReferenceDate - self.firstTokenTime
+                                if elapsed > 0 && self.totalTokens > 0 {
+                                    self.tokensPerSecond = Double(self.totalTokens) / elapsed
+                                    self.totalTime = elapsed
+                                }
+                            }
+                        }
+                    }
+
+                    var generateTokens: Double = 1
+                    var pendingToolCall: ToolCall?
+
+                    // Check if first token is a tool call
+                    if let toolCall = first.toolCall {
+                        pendingToolCall = toolCall
+                    } else if let chunk = first.chunk {
+                        if !chunk.isEmpty {
+                            Task { @MainActor [chunk] in
+                                self.output += chunk
+                                self.totalTokens += 1
+                            }
+                        }
+                    }
+
+                    // Only continue iterating if we haven't hit a tool call
+                    if pendingToolCall == nil {
+                        while let next = await iterator.next() {
+                            // Check for tool calls
+                            if let toolCall = next.toolCall {
+                                pendingToolCall = toolCall
+                                break
+                            }
+
+                            // Handle text chunks
+                            if let chunk = next.chunk {
+                                if !chunk.isEmpty {
+                                    Task { @MainActor [chunk] in
+                                        self.output += chunk
+                                        self.totalTokens += 1
+                                    }
+                                    generateTokens += 1
+                                }
+                            }
+                        }
+                    }
+                    let secondTick = Date.timeIntervalSinceReferenceDate
+                    let generateTime = secondTick - firstTick
+                    let generateTps = generateTokens / generateTime
+
+                    Task { @MainActor in
+                        self.generationTimer?.invalidate()
+                        self.generationTimer = nil
+                        self.tokensPerSecond = generateTps
+                        self.totalTime = generateTime
+
+                        // Check if generation was truncated due to max tokens
+                        if self.totalTokens >= parameters.maxTokens ?? Int.max {
+                            self.wasTruncated = true
+                        }
+                    }
+
+                    // Handle tool call if one was made
+                    if let toolCall = pendingToolCall {
+                        await self.executeToolAndContinue(
+                            toolCall: toolCall, originalPrompt: prompt)
+                    }
+                }
+            }
+
+        } catch {
+            ttftTimer?.invalidate()
+            ttftTimer = nil
+            generationTimer?.invalidate()
+            generationTimer = nil
+            output = "Failed: \(error)"
+        }
+    }
+
+    private func executeToolAndContinue(toolCall: ToolCall, originalPrompt: String) async {
+        // Show tool execution in output
+        self.output += "\n\n[Executing tool: \(toolCall.function.name)...]\n\n"
+
+        let result: String
+        do {
+            result = try await toolExecutor.execute(toolCall)
+        } catch {
+            result = "Error executing tool: \(error.localizedDescription)"
+        }
+
+        // Continue generation with tool result
+        await generate(prompt: originalPrompt, toolResult: result)
+    }
+
+    func generate() {
+        guard !running else { return }
+
+        let currentPrompt = prompt
+        guard !currentPrompt.isEmpty else { return }
+
+        generationTask = Task {
+            running = true
+            await generate(prompt: currentPrompt)
+            prompt = ""
+            running = false
+        }
+    }
+
+    func cancelGeneration() {
+        generationTask?.cancel()
+        ttftTimer?.invalidate()
+        ttftTimer = nil
+        generationTimer?.invalidate()
+        generationTimer = nil
+        running = false
+    }
+}
diff --git a/Applications/LLMEval/Views/ContentView.swift b/Applications/LLMEval/Views/ContentView.swift
new file mode 100644
index 0000000..76a397f
--- /dev/null
+++ b/Applications/LLMEval/Views/ContentView.swift
@@ -0,0 +1,125 @@
+// Copyright © 2025 Apple Inc.
+
+import AsyncAlgorithms
+import MLX
+import MLXLLM
+import MLXLMCommon
+import Metal
+import SwiftUI
+import Tokenizers
+
+struct ContentView: View {
+    @Environment(DeviceStat.self) private var deviceStat
+
+    @State var llm = LLMEvaluator()
+
+    enum DisplayStyle: String, CaseIterable, Identifiable {
+        case plain, markdown
+        var id: Self { self }
+    }
+
+    @State private var selectedDisplayStyle = DisplayStyle.markdown
+    @State private var showingPresetPrompts = false
+    @State private var isPromptExpanded = false
+
+    var body: some View {
+        VStack(alignment: .leading, spacing: 0) {
+            // Header Section
+            HeaderView(
+                llm: llm,
+                selectedDisplayStyle: $selectedDisplayStyle
+            )
+
+            Divider()
+                .padding(.bottom, 12)
+
+            // Output display
+            OutputView(
+                output: llm.output,
+                displayStyle: selectedDisplayStyle,
+                wasTruncated: llm.wasTruncated
+            )
+
+            // Prompt input section
+            PromptInputView(
+                llm: llm,
+                isPromptExpanded: $isPromptExpanded,
+                showingPresetPrompts: $showingPresetPrompts,
+                onGenerate: generate,
+                onCancel: cancel
+            )
+
+            // Performance Metrics Panel
+            MetricsView(
+                tokensPerSecond: llm.tokensPerSecond,
+                timeToFirstToken: llm.timeToFirstToken,
+                promptLength: llm.promptLength,
+                totalTokens: llm.totalTokens,
+                totalTime: llm.totalTime,
+                memoryUsed: deviceStat.gpuUsage.activeMemory,
+                cacheMemory: deviceStat.gpuUsage.cacheMemory,
+                peakMemory: deviceStat.gpuUsage.peakMemory
+            )
+        }
+        #if os(visionOS)
+            .padding(40)
+        #else
+            .padding()
+        #endif
+        .toolbar {
+            ToolbarItem(placement: .primaryAction) {
+                Button {
+                    Task {
+                        copyToClipboard(llm.output)
+                    }
+                } label: {
+                    Label("Copy Output", systemImage: "doc.on.doc.fill")
+                }
+                .disabled(llm.output == "")
+                .labelStyle(.titleAndIcon)
+            }
+
+        }
+        .task {
+            do {
+                // pre-load the weights on launch to speed up the first generation
+                _ = try await llm.load()
+            } catch {
+                llm.output = "Failed: \(error)"
+            }
+        }
+        .sheet(isPresented: $showingPresetPrompts) {
+            PresetPromptsSheet(isPresented: $showingPresetPrompts) { preset in
+                llm.prompt = preset.prompt
+                llm.includeWeatherTool = preset.enableTools
+                llm.enableThinking = preset.enableThinking
+            }
+        }
+        .overlay {
+            if llm.isLoading {
+                LoadingOverlayView(
+                    modelInfo: llm.modelInfo,
+                    downloadProgress: llm.downloadProgress,
+                    progressDescription: llm.totalSize
+                )
+            }
+        }
+    }
+
+    private func generate() {
+        llm.generate()
+    }
+
+    private func cancel() {
+        llm.cancelGeneration()
+    }
+
+    private func copyToClipboard(_ string: String) {
+        #if os(macOS)
+            NSPasteboard.general.clearContents()
+            NSPasteboard.general.setString(string, forType: .string)
+        #else
+            UIPasteboard.general.string = string
+        #endif
+    }
+}
diff --git a/Applications/LLMEval/Views/HeaderView.swift b/Applications/LLMEval/Views/HeaderView.swift
new file mode 100644
index 0000000..1b1323d
--- /dev/null
+++ b/Applications/LLMEval/Views/HeaderView.swift
@@ -0,0 +1,83 @@
+// Copyright © 2025 Apple Inc.
+
+import SwiftUI
+
+struct HeaderView: View {
+    @Bindable var llm: LLMEvaluator
+    @Binding var selectedDisplayStyle: ContentView.DisplayStyle
+
+    var body: some View {
+        VStack(alignment: .leading, spacing: 12) {
+            // Model info with status
+            HStack {
+                VStack(alignment: .leading, spacing: 4) {
+                    Text("Model")
+                        .font(.caption)
+                        .foregroundStyle(.secondary)
+
+                    Text(llm.modelInfo)
+                        .font(.headline)
+                        .lineLimit(1)
+                }
+
+                Spacer()
+
+                if llm.running {
+                    HStack(spacing: 8) {
+                        ProgressView()
+                            .controlSize(.small)
+                        Text("Generating...")
+                            .font(.subheadline)
+                            .foregroundStyle(.secondary)
+                    }
+                }
+            }
+
+            // Controls row
+            HStack(spacing: 16) {
+                HStack(spacing: 24) {
+                    Toggle("Tools", isOn: $llm.includeWeatherTool)
+                        .toggleStyle(.switch)
+                        .fixedSize()
+                        .help("Enable function calling with weather, math, and time tools")
+
+                    Toggle("Thinking", isOn: $llm.enableThinking)
+                        .toggleStyle(.switch)
+                        .fixedSize()
+                        .help("Enable thinking mode (supported by Qwen3)")
+
+                    // Max tokens slider
+                    VStack(alignment: .leading, spacing: 4) {
+                        Text("Max Tokens: \(llm.maxTokens)")
+                            .font(.caption)
+                            .foregroundStyle(.secondary)
+
+                        Slider(
+                            value: Binding(
+                                get: { log2(Double(llm.maxTokens)) },
+                                set: { llm.maxTokens = Int(pow(2, $0)) }
+                            ),
+                            in: 10 ... 15,  // 2^10 (1024) to 2^15 (32768)
+                            step: 1
+                        )
+                        .frame(width: 120)
+                        .help("Maximum number of tokens to generate (1024-32768)")
+                    }
+                }
+
+                Spacer()
+
+                Picker("Display", selection: $selectedDisplayStyle) {
+                    ForEach(ContentView.DisplayStyle.allCases, id: \.self) { option in
+                        Text(option.rawValue.capitalized)
+                            .tag(option)
+                    }
+                }
+                .pickerStyle(.segmented)
+                .labelsHidden()
+                .frame(maxWidth: 180)
+            }
+        }
+        .padding(.bottom, 12)
+    }
+}
diff --git a/Applications/LLMEval/Views/LoadingOverlayView.swift b/Applications/LLMEval/Views/LoadingOverlayView.swift
new file mode 100644
index 0000000..c7a3924
--- /dev/null
+++ b/Applications/LLMEval/Views/LoadingOverlayView.swift
@@ -0,0 +1,55 @@
+// Copyright © 2025 Apple Inc.
+
+import SwiftUI
+
+struct LoadingOverlayView: View {
+    let modelInfo: String
+    let downloadProgress: Double?
+    let progressDescription: String?
+
+    init(modelInfo: String, downloadProgress: Double? = nil, progressDescription: String? = nil) {
+        self.modelInfo = modelInfo
+        self.downloadProgress = downloadProgress
+        self.progressDescription = progressDescription
+    }
+
+    var body: some View {
+        ZStack {
+            Color.black.opacity(0.4)
+                .ignoresSafeArea()
+
+            VStack(spacing: 16) {
+                if let progress = downloadProgress, progress < 1.0 {
+                    ProgressView(value: progress)
+                        .progressViewStyle(.linear)
+                        .frame(width: 200)
+                } else {
+                    ProgressView()
+                        .scaleEffect(1.5)
+                        .progressViewStyle(.circular)
+                }
+
+                Text(modelInfo)
+                    .font(.headline)
+                    .foregroundStyle(.primary)
+
+                if let description = progressDescription {
+                    Text(description)
+                        .font(.subheadline)
+                        .foregroundStyle(.secondary)
+                        .monospacedDigit()
+                }
+
+                Text(
+                    "Models are large and may take a couple of minutes to download on first use. They are cached locally for faster loading in the future."
+                )
+                .font(.subheadline)
+                .foregroundStyle(.secondary)
+                .multilineTextAlignment(.center)
+                .frame(maxWidth: 300)
+            }
+            .padding(32)
+            .background(.regularMaterial, in: RoundedRectangle(cornerRadius: 16))
+        }
+    }
+}
diff --git a/Applications/LLMEval/Views/MetricCard.swift b/Applications/LLMEval/Views/MetricCard.swift
new file mode 100644
index 0000000..237e066
--- /dev/null
+++ b/Applications/LLMEval/Views/MetricCard.swift
@@ -0,0 +1,31 @@
+// Copyright © 2025 Apple Inc.
+
+import SwiftUI
+
+struct MetricCard: View {
+    let icon: String
+    let title: String
+    let value: String
+
+    var body: some View {
+        VStack(spacing: 8) {
+            HStack(spacing: 4) {
+                Image(systemName: icon)
+                    .font(.caption)
+                    .foregroundStyle(.secondary)
+                Text(title)
+                    .font(.caption)
+                    .foregroundStyle(.secondary)
+            }
+            Text(value)
+                .font(.title3)
+                .fontWeight(.semibold)
+                .monospacedDigit()
+        }
+        .frame(maxWidth: .infinity)
+        .padding(.vertical, 12)
+        .padding(.horizontal, 8)
+        .background(Color.gray.opacity(0.1))
+        .cornerRadius(8)
+    }
+}
diff --git a/Applications/LLMEval/Views/MetricsView.swift b/Applications/LLMEval/Views/MetricsView.swift
new file mode 100644
index 0000000..9049df9
--- /dev/null
+++ b/Applications/LLMEval/Views/MetricsView.swift
@@ -0,0 +1,91 @@
+// Copyright © 2025 Apple Inc.
+
+import MLX
+import SwiftUI
+
+struct MetricsView: View {
+    let tokensPerSecond: Double
+    let timeToFirstToken: Double
+    let promptLength: Int
+    let totalTokens: Int
+    let totalTime: Double
+    let memoryUsed: Int
+    let cacheMemory: Int
+    let peakMemory: Int
+
+    @State private var showMemoryDetails = false
+
+    var body: some View {
+        VStack(spacing: 12) {
+            // Top row
+            HStack(spacing: 12) {
+                MetricCard(
+                    icon: "speedometer",
+                    title: "Tokens/sec",
+                    value: String(format: "%.1f", tokensPerSecond)
+                )
+                MetricCard(
+                    icon: "timer",
+                    title: "Time to First Token",
+                    value: String(format: "%.0fms", timeToFirstToken)
+                )
+                MetricCard(
+                    icon: "text.alignleft",
+                    title: "Prompt Length",
+                    value: "\(promptLength)"
+                )
+            }
+
+            // Bottom row
+            HStack(spacing: 12) {
+                MetricCard(
+                    icon: "number",
+                    title: "Total Tokens",
+                    value: "\(totalTokens)"
+                )
+                MetricCard(
+                    icon: "hourglass",
+                    title: "Total Time",
+                    value: String(format: "%.1fs", totalTime)
+                )
+                ZStack(alignment: .topTrailing) {
+                    MetricCard(
+                        icon: "memorychip",
+                        title: "Memory",
+                        value: FormatUtilities.formatMemory(memoryUsed)
+                    )
+                    Button(action: {
+                        #if os(iOS)
+                            showMemoryDetails = true
+                        #endif
+                    }) {
+                        Image(systemName: "info.circle.fill")
+                            .font(.caption)
+                            .foregroundStyle(.secondary)
+                            .frame(width: 44, height: 44)
+                            .contentShape(Rectangle())
+                    }
+                    .buttonStyle(.plain)
+                    .help(
+                        """
+                        Active Memory: \(FormatUtilities.formatMemory(memoryUsed))/\(FormatUtilities.formatMemory(GPU.memoryLimit))
+                        Cache Memory: \(FormatUtilities.formatMemory(cacheMemory))/\(FormatUtilities.formatMemory(GPU.cacheLimit))
+                        Peak Memory: \(FormatUtilities.formatMemory(peakMemory))
+                        """
+                    )
+                }
+            }
+        }
+        .padding(.top, 8)
+        .alert("Memory Details", isPresented: $showMemoryDetails) {
+            Button("OK", role: .cancel) {}
+        } message: {
+            Text(
+                """
+                Active Memory: \(FormatUtilities.formatMemory(memoryUsed))/\(FormatUtilities.formatMemory(GPU.memoryLimit))
+                Cache Memory: \(FormatUtilities.formatMemory(cacheMemory))/\(FormatUtilities.formatMemory(GPU.cacheLimit))
+                Peak Memory: \(FormatUtilities.formatMemory(peakMemory))
+                """)
+        }
+    }
+}
diff --git a/Applications/LLMEval/Views/OutputView.swift b/Applications/LLMEval/Views/OutputView.swift
new file mode 100644
index 0000000..e7c1a72
--- /dev/null
+++ b/Applications/LLMEval/Views/OutputView.swift
@@ -0,0 +1,48 @@
+// Copyright © 2025 Apple Inc.
+
+import MarkdownUI
+import SwiftUI
+
+struct OutputView: View {
+    let output: String
+    let displayStyle: ContentView.DisplayStyle
+    let wasTruncated: Bool
+
+    var body: some View {
+        ScrollView(.vertical) {
+            ScrollViewReader { sp in
+                VStack(alignment: .leading, spacing: 12) {
+                    Group {
+                        if displayStyle == .plain {
+                            Text(output)
+                                .textSelection(.enabled)
+                        } else {
+                            Markdown(output)
+                                .textSelection(.enabled)
+                        }
+                    }
+
+                    // Warning banner when output is truncated
+                    if wasTruncated && !output.isEmpty {
+                        HStack(spacing: 8) {
+                            Image(systemName: "exclamationmark.triangle.fill")
+                                .foregroundStyle(.orange)
+                            Text("Output truncated: Maximum token limit reached")
+                                .font(.caption)
+                                .foregroundStyle(.secondary)
+                        }
+                        .padding(8)
+                        .background(.orange.opacity(0.1), in: RoundedRectangle(cornerRadius: 6))
+                    }
+                }
+                .onChange(of: output) { _, _ in
+                    sp.scrollTo("bottom")
+                }
+
+                Spacer()
+                    .frame(width: 1, height: 1)
+                    .id("bottom")
+            }
+        }
+    }
+}
diff --git a/Applications/LLMEval/Views/PresetPromptsSheet.swift b/Applications/LLMEval/Views/PresetPromptsSheet.swift
new file mode 100644
index 0000000..d90eefe
--- /dev/null
+++ b/Applications/LLMEval/Views/PresetPromptsSheet.swift
@@ -0,0 +1,99 @@
+// Copyright © 2025 Apple Inc.
+
+import SwiftUI
+
+struct PresetPromptsSheet: View {
+    @Binding var isPresented: Bool
+    let onSelect: (PresetPrompt) -> Void
+
+    var body: some View {
+        NavigationStack {
+            List {
+                ForEach(PresetPrompts.all) { preset in
+                    Button {
+                        onSelect(preset)
+                        isPresented = false
+                    } label: {
+                        HStack(alignment: .center, spacing: 12) {
+                            // Show the actual prompt (first 2 lines)
+                            // Clean up whitespace for better preview
+                            let cleanedPrompt = preset.prompt
+                                .trimmingCharacters(in: .whitespacesAndNewlines)
+                                .replacingOccurrences(
+                                    of: #"\n\n+"#, with: " ", options: .regularExpression
+                                )
+                                .replacingOccurrences(
+                                    of: #"\s+"#, with: " ", options: .regularExpression)
+
+                            Text(cleanedPrompt)
+                                .multilineTextAlignment(.leading)
+                                .lineLimit(2)
+                                .font(.body)
+                                .foregroundStyle(.primary)
+                                .frame(maxWidth: .infinity, alignment: .leading)
+
+                            // Show indicators if present
+                            if preset.enableThinking || preset.enableTools || preset.isLongPrompt {
+                                HStack(spacing: 6) {
+                                    if preset.enableThinking {
+                                        BadgeView(icon: "brain", text: "Thinking", color: .purple)
+                                    }
+                                    if preset.enableTools {
+                                        BadgeView(icon: "hammer.fill", text: "Tools", color: .blue)
+                                    }
+                                    if preset.isLongPrompt {
+                                        BadgeView(
+                                            icon: "doc.text.fill", text: "Long", color: .orange)
+                                    }
+                                }
+                            }
+                        }
+                        .padding(.vertical, 8)
+                        #if os(macOS)
+                            .frame(maxWidth: .infinity, alignment: .leading)
+                            .contentShape(Rectangle())
+                        #endif
+                    }
+                    .buttonStyle(.plain)
+                    #if os(macOS)
+                        .listRowInsets(EdgeInsets(top: 8, leading: 12, bottom: 8, trailing: 12))
+                    #endif
+                }
+            }
+            #if os(macOS)
+                .listStyle(.inset)
+            #endif
+            .navigationTitle("Example Prompts")
+            #if !os(macOS)
+                .navigationBarTitleDisplayMode(.inline)
+            #endif
+            .toolbar {
+                ToolbarItem(placement: .cancellationAction) {
+                    Button("Close") {
+                        isPresented = false
+                    }
+                }
+            }
+        }
+        #if os(macOS)
+            .frame(minWidth: 600, minHeight: 500)
+        #endif
+    }
+}
+
+// Badge component
+private struct BadgeView: View {
+    let icon: String
+    let text: String
+    let color: Color
+
+    var body: some View {
+        Label(text, systemImage: icon)
+            .font(.caption2)
+            .fontWeight(.medium)
+            .foregroundStyle(.white)
+            .padding(.horizontal, 8)
+            .padding(.vertical, 4)
+            .background(color.gradient, in: Capsule())
+    }
+}
diff --git a/Applications/LLMEval/Views/PromptInputView.swift b/Applications/LLMEval/Views/PromptInputView.swift
new file mode 100644
index 0000000..f5f318f
--- /dev/null
+++ b/Applications/LLMEval/Views/PromptInputView.swift
@@ -0,0 +1,74 @@
+// Copyright © 2025 Apple Inc.
+
+import SwiftUI
+
+struct PromptInputView: View {
+    @Bindable var llm: LLMEvaluator
+    @Binding var isPromptExpanded: Bool
+    @Binding var showingPresetPrompts: Bool
+
+    let onGenerate: () -> Void
+    let onCancel: () -> Void
+
+    var body: some View {
+        VStack(alignment: .leading, spacing: 8) {
+            // Prompt header with expand/collapse chevron
+            HStack {
+                Text("Prompt")
+                    .font(.caption)
+                    .foregroundStyle(.secondary)
+
+                Spacer()
+
+                Button {
+                    withAnimation(.easeInOut(duration: 0.2)) {
+                        isPromptExpanded.toggle()
+                    }
+                } label: {
+                    Image(systemName: isPromptExpanded ? "chevron.down" : "chevron.up")
+                        .font(.caption)
+                        .foregroundStyle(.secondary)
+                }
+                .buttonStyle(.plain)
+                .help(isPromptExpanded ? "Collapse prompt area" : "Expand prompt area")
+            }
+
+            // Prompt text field with dynamic sizing
+            TextField("Enter your prompt...", text: $llm.prompt, axis: .vertical)
+                .textFieldStyle(.roundedBorder)
+                .lineLimit(isPromptExpanded ? 15 ... 50 : 1 ... 3)
+                .frame(height: isPromptExpanded ? 400 : nil)
+                .onSubmit(onGenerate)
+                .disabled(llm.running || llm.isLoading)
+
+            // Action buttons
+            HStack(spacing: 12) {
+                Button {
+                    showingPresetPrompts = true
+                } label: {
+                    Label("Example Prompts", systemImage: "list.bullet")
+                }
+                .disabled(llm.running || llm.isLoading)
+
+                Spacer()
+
+                Button {
+                    if llm.running {
+                        onCancel()
+                    } else {
+                        onGenerate()
+                    }
+                } label: {
+                    Label(
+                        llm.running ? "Stop" : "Generate",
+                        systemImage: llm.running ? "stop.circle" : "play.fill"
+                    )
+                }
+                .buttonStyle(.borderedProminent)
+                .keyboardShortcut(.return, modifiers: .command)
+                .disabled((llm.prompt.isEmpty && !llm.running) || llm.isLoading)
+            }
+        }
+        .padding(.vertical, 8)
+    }
+}
diff --git a/mlx-swift-examples.xcodeproj/project.pbxproj b/mlx-swift-examples.xcodeproj/project.pbxproj
index 0a09ef9..5230c01 100644
--- a/mlx-swift-examples.xcodeproj/project.pbxproj
+++ b/mlx-swift-examples.xcodeproj/project.pbxproj
@@ -14,12 +14,12 @@
 		0AC74ED02D13622A003C90A7 /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = 0AC74ECB2D13622A003C90A7 /* Assets.xcassets */; };
 		0AC74ED12D13622A003C90A7 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 0AC74ECC2D13622A003C90A7 /* ContentView.swift */; };
 		0AC74ED22D13622A003C90A7 /* VLMEvalApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = 0AC74ECD2D13622A003C90A7 /* VLMEvalApp.swift */; };
-		0AC74ED32D136265003C90A7 /* DeviceStat.swift in Sources */ = {isa = PBXBuildFile; fileRef = 819BEFF62BAF8B4E0002CCEE /* DeviceStat.swift */; };
 		0F5AD8002DB70E6300745C06 /* MLXLLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A17FC2CFFB98A0092A5B6 /* MLXLLM */; };
 		0F5AD8012DB70E6300745C06 /* MLXVLM in Frameworks */ = {isa = PBXBuildFile; productRef = C32A17FE2CFFB98A0092A5B6 /* MLXVLM */; };
 		12305EAF2B9D864400C92FEE /* PredictionView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 12305EAE2B9D864400C92FEE /* PredictionView.swift */; };
+		6C5EF5D82EEA33FA00134C83 /* AppIcon.icon in Resources */ = {isa = PBXBuildFile; fileRef = 6C5EF5D72EEA33FA00134C83 /* AppIcon.icon */; };
+		6C5EF5DC2EEA35B700134C83 /* AssetCatalog.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = 6C5EF5DB2EEA35B700134C83 /* AssetCatalog.xcassets */; };
 		81695B412BA373D300F260D8 /* MarkdownUI in Frameworks */ = {isa = PBXBuildFile; productRef = 81695B402BA373D300F260D8 /* MarkdownUI */; };
-		819BEFF82BAF8B4E0002CCEE /* DeviceStat.swift in Sources */ = {isa = PBXBuildFile; fileRef = 819BEFF62BAF8B4E0002CCEE /* DeviceStat.swift */; };
 		C3056BAE2BCD97B700A31D04 /* LoRATrainingExampleApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3056BAD2BCD97B700A31D04 /* LoRATrainingExampleApp.swift */; };
 		C3056BB02BCD97B700A31D04 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3056BAF2BCD97B700A31D04 /* ContentView.swift */; };
 		C3056BB22BCD97B800A31D04 /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3056BB12BCD97B800A31D04 /* Assets.xcassets */; };
@@ -70,10 +70,7 @@
 		C3A8B3CC2B92951E0002EFB8 /* MNISTTrainerApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3A8B3C42B92951E0002EFB8 /* MNISTTrainerApp.swift */; };
 		C3A8B3CD2B92951E0002EFB8 /* Preview Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3A8B3C62B92951E0002EFB8 /* Preview Assets.xcassets */; };
 		C3A8B3CF2B92951E0002EFB8 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3A8B3C92B92951E0002EFB8 /* ContentView.swift */; };
-		C3A8B3F32B92A2A90002EFB8 /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3A8B3EC2B92A2A90002EFB8 /* Assets.xcassets */; };
 		C3A8B3F42B92A2A90002EFB8 /* LLMEvalApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3A8B3ED2B92A2A90002EFB8 /* LLMEvalApp.swift */; };
-		C3A8B3F52B92A2A90002EFB8 /* Preview Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = C3A8B3EF2B92A2A90002EFB8 /* Preview Assets.xcassets */; };
-		C3A8B3F72B92A2A90002EFB8 /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = C3A8B3F22B92A2A90002EFB8 /* ContentView.swift */; };
 		C3C7C4FB2DB19026000373CF /* AVKit.framework in Frameworks */ = {isa = PBXBuildFile; fileRef = C3C7C4FA2DB19026000373CF /* AVKit.framework */; };
 		C3E7D94D2CF6C9B20056C095 /* StableDiffusion in Frameworks */ = {isa = PBXBuildFile; productRef = C3E7D94C2CF6C9B20056C095 /* StableDiffusion */; };
 /* End PBXBuildFile section */
@@ -225,7 +222,8 @@
 		0AC74F282D1376F1003C90A7 /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 		0F5AD7412DB70C0300745C06 /* MLXChatExample.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = MLXChatExample.app; sourceTree = BUILT_PRODUCTS_DIR; };
 		12305EAE2B9D864400C92FEE /* PredictionView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = PredictionView.swift; sourceTree = "<group>"; };
-		819BEFF62BAF8B4E0002CCEE /* DeviceStat.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = DeviceStat.swift; sourceTree = "<group>"; };
+		6C5EF5D72EEA33FA00134C83 /* AppIcon.icon */ = {isa = PBXFileReference; lastKnownFileType = folder.iconcomposer.icon; path = AppIcon.icon; sourceTree = "<group>"; };
+		6C5EF5DB2EEA35B700134C83 /* AssetCatalog.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = AssetCatalog.xcassets; sourceTree = "<group>"; };
 		C3056BA12BCD973400A31D04 /* test.jsonl */ = {isa = PBXFileReference; lastKnownFileType = text; path = test.jsonl; sourceTree = "<group>"; };
 		C3056BA22BCD973400A31D04 /* train.jsonl */ = {isa = PBXFileReference; lastKnownFileType = text; path = train.jsonl; sourceTree = "<group>"; };
 		C3056BA32BCD973400A31D04 /* valid.jsonl */ = {isa = PBXFileReference; lastKnownFileType = text; path = valid.jsonl; sourceTree = "<group>"; };
@@ -274,12 +272,9 @@
 		C3A8B3C82B92951E0002EFB8 /* README.md */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 		C3A8B3C92B92951E0002EFB8 /* ContentView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = ContentView.swift; sourceTree = "<group>"; };
 		C3A8B3DC2B92A29E0002EFB8 /* LLMEval.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = LLMEval.app; sourceTree = BUILT_PRODUCTS_DIR; };
-		C3A8B3EC2B92A2A90002EFB8 /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
 		C3A8B3ED2B92A2A90002EFB8 /* LLMEvalApp.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMEvalApp.swift; sourceTree = "<group>"; };
-		C3A8B3EF2B92A2A90002EFB8 /* Preview Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = "Preview Assets.xcassets"; sourceTree = "<group>"; };
 		C3A8B3F02B92A2A90002EFB8 /* README.md */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 		C3A8B3F12B92A2A90002EFB8 /* LLMEval.entitlements */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.plist.entitlements; path = LLMEval.entitlements; sourceTree = "<group>"; };
-		C3A8B3F22B92A2A90002EFB8 /* ContentView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = ContentView.swift; sourceTree = "<group>"; };
 		C3C3240B2B6CA689007D2D9A /* README.md */ = {isa = PBXFileReference; lastKnownFileType = net.daringfireball.markdown; path = README.md; sourceTree = "<group>"; };
 		C3C36A6B2CA714600099FFA4 /* Build.xcconfig */ = {isa = PBXFileReference; lastKnownFileType = text.xcconfig; path = Build.xcconfig; sourceTree = "<group>"; };
 		C3C7C4D92DB16C83000373CF /* AVFoundation.framework */ = {isa = PBXFileReference; lastKnownFileType = wrapper.framework; name = AVFoundation.framework; path = Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS18.4.sdk/System/Library/Frameworks/AVFoundation.framework; sourceTree = DEVELOPER_DIR; };
@@ -307,6 +302,10 @@
 /* Begin PBXFileSystemSynchronizedRootGroup section */
 		0A979CD22E97175800635D67 /* embedder-tool */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = "embedder-tool"; sourceTree = "<group>"; };
 		0F5AD7422DB70C0300745C06 /* MLXChatExample */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = MLXChatExample; sourceTree = "<group>"; };
+		6C5EF5A82EEA33B000134C83 /* Models */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = Models; sourceTree = "<group>"; };
+		6C5EF5BE2EEA33CD00134C83 /* ViewModels */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = ViewModels; sourceTree = "<group>"; };
+		6C5EF5C92EEA33D900134C83 /* Views */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = Views; sourceTree = "<group>"; };
+		6C5EF5D42EEA33E800134C83 /* Services */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = Services; sourceTree = "<group>"; };
 		C397D92E2CD440EF00B87EE2 /* Libraries */ = {isa = PBXFileSystemSynchronizedRootGroup; explicitFileTypes = {}; explicitFolders = (); path = Libraries; sourceTree = "<group>"; };
 		C3C7C4222DB16A02000373CF /* Tests */ = {isa = PBXFileSystemSynchronizedRootGroup; exceptions = (C3208F272DB19614006AE6CA /* PBXFileSystemSynchronizedBuildFileExceptionSet */, ); explicitFileTypes = {}; explicitFolders = (); path = Tests; sourceTree = "<group>"; };
 /* End PBXFileSystemSynchronizedRootGroup section */
@@ -466,14 +465,6 @@
 			path = VLMEval;
 			sourceTree = "<group>";
 		};
-		819BEFF72BAF8B4E0002CCEE /* ViewModels */ = {
-			isa = PBXGroup;
-			children = (
-				819BEFF62BAF8B4E0002CCEE /* DeviceStat.swift */,
-			);
-			path = ViewModels;
-			sourceTree = "<group>";
-		};
 		C3056BA52BCD973400A31D04 /* lora */ = {
 			isa = PBXGroup;
 			children = (
@@ -693,25 +684,19 @@
 		C3A8B3EB2B92A2A90002EFB8 /* LLMEval */ = {
 			isa = PBXGroup;
 			children = (
-				819BEFF72BAF8B4E0002CCEE /* ViewModels */,
-				C3A8B3EC2B92A2A90002EFB8 /* Assets.xcassets */,
-				C3A8B3F22B92A2A90002EFB8 /* ContentView.swift */,
+				6C5EF5C92EEA33D900134C83 /* Views */,
+				6C5EF5D42EEA33E800134C83 /* Services */,
+				6C5EF5A82EEA33B000134C83 /* Models */,
+				6C5EF5BE2EEA33CD00134C83 /* ViewModels */,
+				6C5EF5DB2EEA35B700134C83 /* AssetCatalog.xcassets */,
 				C3A8B3F12B92A2A90002EFB8 /* LLMEval.entitlements */,
 				C3A8B3ED2B92A2A90002EFB8 /* LLMEvalApp.swift */,
-				C3A8B3EE2B92A2A90002EFB8 /* Preview Content */,
 				C3A8B3F02B92A2A90002EFB8 /* README.md */,
+				6C5EF5D72EEA33FA00134C83 /* AppIcon.icon */,
 			);
 			path = LLMEval;
 			sourceTree = "<group>";
 		};
-		C3A8B3EE2B92A2A90002EFB8 /* Preview Content */ = {
-			isa = PBXGroup;
-			children = (
-				C3A8B3EF2B92A2A90002EFB8 /* Preview Assets.xcassets */,
-			);
-			path = "Preview Content";
-			sourceTree = "<group>";
-		};
 		C3C36A6C2CA714600099FFA4 /* Configuration */ = {
 			isa = PBXGroup;
 			children = (
@@ -1018,6 +1003,12 @@
 			);
 			dependencies = (
 			);
+			fileSystemSynchronizedGroups = (
+				6C5EF5A82EEA33B000134C83 /* Models */,
+				6C5EF5BE2EEA33CD00134C83 /* ViewModels */,
+				6C5EF5C92EEA33D900134C83 /* Views */,
+				6C5EF5D42EEA33E800134C83 /* Services */,
+			);
 			name = LLMEval;
 			packageProductDependencies = (
 				81695B402BA373D300F260D8 /* MarkdownUI */,
@@ -1175,8 +1166,8 @@
 			isa = PBXResourcesBuildPhase;
 			buildActionMask = 2147483647;
 			files = (
-				C3A8B3F52B92A2A90002EFB8 /* Preview Assets.xcassets in Resources */,
-				C3A8B3F32B92A2A90002EFB8 /* Assets.xcassets in Resources */,
+				6C5EF5D82EEA33FA00134C83 /* AppIcon.icon in Resources */,
+				6C5EF5DC2EEA35B700134C83 /* AssetCatalog.xcassets in Resources */,
 			);
 			runOnlyForDeploymentPostprocessing = 0;
 		};
@@ -1195,7 +1186,6 @@
 			buildActionMask = 2147483647;
 			files = (
 				0AC74ED12D13622A003C90A7 /* ContentView.swift in Sources */,
-				0AC74ED32D136265003C90A7 /* DeviceStat.swift in Sources */,
 				0AC74ED22D13622A003C90A7 /* VLMEvalApp.swift in Sources */,
 			);
 			runOnlyForDeploymentPostprocessing = 0;
@@ -1300,8 +1290,6 @@
 			buildActionMask = 2147483647;
 			files = (
 				C3A8B3F42B92A2A90002EFB8 /* LLMEvalApp.swift in Sources */,
-				C3A8B3F72B92A2A90002EFB8 /* ContentView.swift in Sources */,
-				819BEFF82BAF8B4E0002CCEE /* DeviceStat.swift in Sources */,
 			);
 			runOnlyForDeploymentPostprocessing = 0;
 		};
@@ -3100,8 +3088,8 @@
 				CURRENT_PROJECT_VERSION = 1;
 				DEAD_CODE_STRIPPING = YES;
 				DEBUG_INFORMATION_FORMAT = dwarf;
-				DEVELOPMENT_ASSET_PATHS = "\"Applications/LLMEval/Preview Content\"";
 				DEVELOPMENT_TEAM = "";
+				ENABLE_OUTGOING_NETWORK_CONNECTIONS = YES;
 				ENABLE_PREVIEWS = YES;
 				ENABLE_STRICT_OBJC_MSGSEND = YES;
 				ENABLE_TESTABILITY = YES;
@@ -3193,9 +3181,9 @@
 				CURRENT_PROJECT_VERSION = 1;
 				DEAD_CODE_STRIPPING = YES;
 				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
-				DEVELOPMENT_ASSET_PATHS = "\"Applications/LLMEval/Preview Content\"";
 				DEVELOPMENT_TEAM = "";
 				ENABLE_NS_ASSERTIONS = NO;
+				ENABLE_OUTGOING_NETWORK_CONNECTIONS = YES;
 				ENABLE_PREVIEWS = YES;
 				ENABLE_STRICT_OBJC_MSGSEND = YES;
 				ENABLE_USER_SCRIPT_SANDBOXING = YES;

commit 7e2e757c90aa340df6621600b12ad678d69f2a8a
Author: David Koski <46639364+davidkoski@users.noreply.github.com>
Date:   Tue Dec 2 13:27:38 2025 -0800

    switch to github actions (#446)
    
    * switch to github actions
    * remove circleci
    
    Co-authored-by: Mike Drob <mdrob@apache.org>

diff --git a/.circleci/config.yml b/.circleci/config.yml
deleted file mode 100644
index 87af9a7..0000000
--- a/.circleci/config.yml
+++ /dev/null
@@ -1,74 +0,0 @@
-version: 2.1
-
-orbs:
-  apple: ml-explore/pr-approval@0.1.0
-
-parameters:
-  nightly_build:
-    type: boolean
-    default: false
-  weekly_build:
-    type: boolean
-    default: false
-
-jobs:
-
-  mac_build_and_test:
-    parameters:
-      xcode_version:
-        type: string
-    macos:
-      xcode: << parameters.xcode_version >>
-    resource_class: macos.m1.medium.gen1
-    steps:
-      - checkout
-      - run: git submodule sync
-      - run: git submodule update --init
-      - run:
-          name: Run style checks
-          command: |
-            pip install pre-commit
-            brew install swift-format
-            pre-commit run --all || (echo "Style checks failed, please install pre-commit and run pre-commit run --all and push the change"; echo ""; git --no-pager diff; exit 1)
-      - run:
-          name: Build Examples
-          command: |
-            xcodebuild -version
-            xcrun --show-sdk-build-version
-            swift --version
-            find . -name Package.resolved -exec rm {} \;
-            xcodebuild -scheme llm-tool
-            xcodebuild -scheme image-tool
-            xcodebuild -scheme mnist-tool
-            xcodebuild -scheme embedder-tool
-
-workflows:
-  build_and_test:
-    when:
-      and:
-        - matches:
-            pattern: "^(?!pull/)[-\\w]+$"
-            value: << pipeline.git.branch >>
-        - not: << pipeline.parameters.nightly_build >>
-        - not: << pipeline.parameters.weekly_build >>
-    jobs:
-      - mac_build_and_test:
-          matrix:
-            parameters:
-              xcode_version: ["16.3.0"]
-
-  prb:
-    when:
-      matches:
-        pattern: "^pull/\\d+(/head)?$"
-        value: << pipeline.git.branch >>
-    jobs:
-      - hold:
-          type: approval
-      - apple/authenticate:
-          context: pr-approval
-      - mac_build_and_test:
-          requires: [ hold ]
-          matrix:
-            parameters:
-              xcode_version: ["16.3.0"]
diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
new file mode 100644
index 0000000..75520f3
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -0,0 +1,29 @@
+---
+name: Bug report
+about: Create a report about an issue you've encountered
+title: "[BUG] "
+labels: ''
+assignees: ''
+
+---
+
+**Describe the bug**
+A clear and concise description of what the bug is.
+
+**To Reproduce**
+
+Include code snippet
+```swift
+
+```
+
+**Expected behavior**
+A clear and concise description of what you expected to happen.
+
+**Desktop (please complete the following information):**
+ - OS Version: [e.g. MacOS 14.1.2]
+ - Device: [e.g. iPhone 16, M2 MacBook Pro]
+ - Version [e.g. 0.29.1]
+
+**Additional context**
+Add any other context about the problem here.
diff --git a/.github/pull_request_template.md b/.github/pull_request_template.md
new file mode 100644
index 0000000..02bb9b7
--- /dev/null
+++ b/.github/pull_request_template.md
@@ -0,0 +1,12 @@
+## Proposed changes
+
+Please include a description of the problem or feature this PR is addressing. If there is a corresponding issue, include the issue #.
+
+## Checklist
+
+Put an `x` in the boxes that apply.
+
+- [ ] I have read the [CONTRIBUTING](https://github.com/ml-explore/mlx/blob/main/CONTRIBUTING.md) document
+- [ ] I have run `pre-commit run --all-files` to format my code / installed pre-commit prior to committing changes
+- [ ] I have added tests that prove my fix is effective or that my feature works
+- [ ] I have updated the necessary documentation (if needed)
diff --git a/.github/workflows/pull_request.yml b/.github/workflows/pull_request.yml
new file mode 100644
index 0000000..eaa0405
--- /dev/null
+++ b/.github/workflows/pull_request.yml
@@ -0,0 +1,104 @@
+name: Build and Test
+
+on: pull_request  
+
+permissions:
+  contents: read
+
+jobs:
+  lint:
+    if: github.repository == 'ml-explore/mlx-swift-examples'
+    runs-on: ubuntu-22.04
+    container:
+      image: swift:6.2-rhel-ubi9
+    steps:
+      - uses: actions/checkout@v6
+        with:
+          submodules: recursive           
+
+      - name: Setup uv
+        uses: astral-sh/setup-uv@v6
+        with:
+            activate-environment: true
+     
+      - name: Setup pre-commit
+        shell: sh
+        run: |
+          uv pip install pre-commit
+
+      - name: Get swift-format tag
+        id: swift-format
+        shell: sh
+        run: |
+          cd /tmp
+          LATEST_TAG=$(curl -s https://api.github.com/repos/swiftlang/swift-format/releases/latest | \
+            grep '"tag_name":' | \
+            sed -E 's/.*"([^"]+)".*/\1/')
+          echo "swift-format $LATEST_TAG"
+          echo "SWIFT_FORMAT_VERSION=$LATEST_TAG" >> $GITHUB_OUTPUT
+
+      - name: Cache swift-format build
+        uses: actions/cache@v4
+        id: cache-swift-format
+        with:
+          path: /tmp/swift-format/.build
+          key: ${{ runner.os }}-swift-format-build-${{ steps.swift-format.outputs.SWIFT_FORMAT_VERSION }}
+
+      - name: Build swift-format
+        if: steps.cache-swift-format.outputs.cache-hit != 'true'
+        shell: sh
+        run: |
+          cd /tmp
+          git clone --branch ${{ steps.swift-format.outputs.SWIFT_FORMAT_VERSION }} --depth 1 https://github.com/swiftlang/swift-format.git
+          cd swift-format
+          swift build -c release
+
+      - name: Link swift-format to /usr/local/bin
+        shell: sh
+        run: |
+          cd /tmp/swift-format
+          ln -s "$(swift build --show-bin-path -c release)/swift-format" /usr/local/bin/swift-format
+     
+      - name: Configure safe directory for git
+        shell: sh
+        run: |
+          git config --global --add safe.directory "$GITHUB_WORKSPACE"
+     
+      - name: Run style checks
+        shell: sh
+        run: |
+          pre-commit run --all || (echo "Style checks failed, please install pre-commit and run pre-commit run --all and push the change"; echo ""; git --no-pager diff; exit 1)
+      
+
+  mac_build_and_test:
+    needs: lint
+    if: github.repository == 'ml-explore/mlx-swift-examples'
+    runs-on: [self-hosted, macos]
+    steps:
+      - uses: actions/checkout@v6
+        with:
+          submodules: recursive
+     
+      - name: Verify MetalToolchain installed
+        shell: bash
+        run: xcodebuild -showComponent MetalToolchain
+            
+      - name: Build Package (Xcode, macOS)
+        shell: sh
+        run: |
+          xcodebuild -version
+          xcrun --show-sdk-build-version
+          swift --version
+          rm -rf ~/Library/Developer/Xcode/DerivedData/*
+          xcodebuild build-for-testing -scheme mlx-libraries-Package -destination 'platform=macOS'
+
+      - name: Build tools (Xcode, macOS)
+        shell: sh
+        run: |
+          xcodebuild -version
+          xcrun --show-sdk-build-version
+          swift --version
+          find . -name Package.resolved -exec rm {} \;
+          xcodebuild -scheme llm-tool
+          xcodebuild -scheme image-tool
+          xcodebuild -scheme mnist-tool
